{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Text analysis using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook , we would be discussing about -\n",
    "* Webscraping\n",
    "* Data cleaning using Natural Language Processing Tools\n",
    "* Machine learning algorithms for prediction of data\n",
    "* Clustering of text using unsupervised models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webscraping is a method to extract data from websites.It is of many types-\n",
    "\n",
    "* Human-copy-and-paste\n",
    "* HTML parsing\n",
    "* Computer Vision webpage analysis\n",
    "\n",
    "We would be using HTML parsing to create our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n",
      "875\n",
      "900\n",
      "925\n",
      "950\n",
      "975\n",
      "1000\n",
      "1025\n",
      "1050\n",
      "1075\n",
      "1100\n",
      "1125\n",
      "1150\n",
      "1175\n",
      "1200\n",
      "1225\n",
      "1250\n",
      "1275\n",
      "1300\n",
      "1325\n",
      "1350\n",
      "1375\n",
      "1400\n",
      "1425\n",
      "1450\n",
      "1475\n",
      "1500\n",
      "1525\n",
      "1550\n",
      "1575\n",
      "1600\n",
      "1625\n",
      "1650\n",
      "1675\n",
      "1700\n",
      "1725\n",
      "1750\n",
      "1775\n",
      "1800\n",
      "1825\n",
      "1850\n",
      "1875\n",
      "1900\n",
      "1925\n",
      "1950\n",
      "1975\n",
      "2000\n",
      "2025\n",
      "2050\n",
      "2075\n",
      "2100\n",
      "2125\n",
      "2150\n",
      "2175\n",
      "2200\n",
      "2225\n",
      "2250\n",
      "2275\n",
      "2300\n",
      "2325\n",
      "2350\n",
      "2375\n",
      "2400\n",
      "2425\n",
      "2450\n",
      "2475\n",
      "2500\n",
      "2525\n",
      "2550\n",
      "2575\n",
      "2600\n",
      "2625\n",
      "2650\n",
      "2675\n",
      "2700\n",
      "2725\n",
      "2750\n",
      "2775\n",
      "2800\n",
      "2825\n",
      "2850\n",
      "2875\n",
      "2900\n",
      "2925\n",
      "2950\n",
      "2975\n",
      "3000\n",
      "3025\n",
      "3050\n",
      "3075\n",
      "3100\n",
      "3125\n",
      "3150\n",
      "3175\n",
      "3200\n",
      "3225\n",
      "3250\n",
      "3275\n",
      "3300\n",
      "3325\n",
      "3350\n",
      "3375\n",
      "3400\n",
      "3425\n",
      "3450\n",
      "3475\n",
      "3500\n",
      "3525\n",
      "3550\n",
      "3575\n",
      "3600\n",
      "3625\n",
      "3650\n",
      "3675\n",
      "3700\n",
      "3725\n",
      "3750\n",
      "3775\n",
      "3800\n",
      "3825\n",
      "3850\n",
      "3875\n",
      "3900\n",
      "3925\n",
      "3950\n",
      "3975\n",
      "4000\n",
      "4025\n",
      "4050\n",
      "4075\n",
      "4100\n",
      "4125\n",
      "4150\n",
      "4175\n",
      "4200\n",
      "4225\n",
      "4250\n",
      "4275\n",
      "4300\n",
      "4325\n",
      "4350\n",
      "4375\n",
      "4400\n",
      "4425\n",
      "4450\n",
      "4475\n",
      "4500\n",
      "4525\n",
      "4550\n",
      "4575\n",
      "4600\n",
      "4625\n",
      "4650\n",
      "4675\n",
      "4700\n",
      "4725\n",
      "4750\n",
      "4775\n",
      "4800\n",
      "4825\n",
      "4850\n",
      "4875\n",
      "4900\n",
      "4925\n",
      "4950\n",
      "4975\n",
      "5000\n",
      "5025\n",
      "5050\n",
      "5075\n",
      "5100\n",
      "5125\n",
      "5150\n",
      "5175\n",
      "5200\n",
      "5225\n",
      "5250\n",
      "5275\n",
      "5300\n",
      "5325\n",
      "5350\n",
      "5375\n",
      "5400\n",
      "5425\n",
      "5450\n",
      "5475\n",
      "5500\n",
      "5525\n",
      "5550\n",
      "5575\n",
      "5600\n",
      "5625\n",
      "5650\n",
      "5675\n",
      "5700\n",
      "5725\n",
      "5750\n",
      "5775\n",
      "5800\n",
      "5825\n",
      "5850\n",
      "5875\n",
      "5900\n",
      "5925\n",
      "5950\n",
      "5975\n",
      "6000\n",
      "6025\n",
      "6050\n",
      "6075\n",
      "6100\n",
      "6125\n",
      "6150\n",
      "6175\n",
      "6200\n",
      "6225\n",
      "6250\n",
      "6275\n",
      "6300\n",
      "6325\n",
      "6350\n",
      "6375\n",
      "6400\n",
      "6425\n",
      "6450\n",
      "6475\n",
      "6500\n",
      "6525\n",
      "6550\n",
      "6575\n",
      "6600\n",
      "6625\n",
      "6650\n",
      "6675\n",
      "6700\n",
      "6725\n",
      "6750\n",
      "6775\n",
      "6800\n",
      "6825\n",
      "6850\n",
      "6875\n",
      "6900\n",
      "6925\n",
      "6950\n",
      "6975\n",
      "7000\n",
      "7025\n",
      "7050\n",
      "7075\n",
      "7100\n",
      "7125\n",
      "7150\n",
      "7175\n",
      "7200\n",
      "7225\n",
      "7250\n",
      "7275\n",
      "7300\n",
      "7325\n",
      "7350\n",
      "7375\n",
      "7400\n",
      "7425\n",
      "7450\n",
      "7475\n",
      "7500\n",
      "7525\n",
      "7550\n",
      "7575\n",
      "7600\n",
      "7625\n",
      "7650\n",
      "7675\n",
      "7700\n",
      "7725\n",
      "7750\n",
      "7775\n",
      "7800\n",
      "7825\n",
      "7850\n",
      "7875\n",
      "7900\n",
      "7925\n",
      "7950\n",
      "7975\n",
      "8000\n",
      "8025\n",
      "8050\n",
      "8075\n",
      "8100\n",
      "8125\n",
      "8150\n",
      "8175\n",
      "8200\n",
      "8225\n",
      "8250\n",
      "8275\n",
      "8300\n",
      "8325\n",
      "8350\n",
      "8375\n",
      "8400\n",
      "8425\n",
      "8450\n",
      "8475\n",
      "8500\n",
      "8525\n",
      "8550\n",
      "8575\n",
      "8600\n",
      "8625\n",
      "8650\n",
      "8675\n",
      "8700\n",
      "8725\n",
      "8750\n",
      "8775\n",
      "8800\n",
      "8825\n",
      "8850\n",
      "8875\n",
      "8900\n",
      "8925\n",
      "8950\n",
      "8975\n",
      "9000\n",
      "9025\n",
      "9050\n",
      "9075\n",
      "9100\n",
      "9125\n",
      "9150\n",
      "9175\n",
      "9200\n",
      "9225\n",
      "9250\n",
      "9275\n",
      "9300\n",
      "9325\n",
      "9350\n",
      "9375\n",
      "9400\n",
      "9425\n",
      "9450\n",
      "9475\n",
      "9500\n",
      "9525\n",
      "9550\n",
      "9575\n",
      "9600\n",
      "9625\n",
      "9650\n",
      "9675\n",
      "9700\n",
      "9725\n",
      "9750\n",
      "9775\n",
      "9800\n",
      "9825\n",
      "9850\n",
      "9875\n",
      "9900\n",
      "9925\n",
      "9950\n",
      "9975\n",
      "10000\n",
      "10025\n",
      "10050\n",
      "10075\n",
      "10100\n",
      "10125\n",
      "10150\n",
      "10175\n",
      "10200\n",
      "10225\n",
      "10250\n",
      "10275\n",
      "10300\n",
      "10325\n",
      "10350\n",
      "10375\n",
      "10400\n",
      "10425\n",
      "10450\n",
      "10475\n",
      "10500\n",
      "10525\n",
      "10550\n",
      "10575\n",
      "10600\n",
      "10625\n",
      "10650\n",
      "10675\n",
      "10700\n",
      "10725\n",
      "10750\n",
      "10775\n",
      "10800\n",
      "10825\n",
      "10850\n",
      "10875\n",
      "10900\n",
      "10925\n",
      "10950\n",
      "10975\n",
      "11000\n",
      "11025\n",
      "11050\n",
      "11075\n",
      "11100\n",
      "11125\n",
      "11150\n",
      "11175\n",
      "11200\n",
      "11225\n",
      "11250\n",
      "11275\n",
      "11300\n",
      "11325\n",
      "11350\n",
      "11375\n",
      "11400\n",
      "11425\n",
      "11450\n",
      "11475\n",
      "11500\n",
      "11525\n",
      "11550\n",
      "11575\n",
      "11600\n",
      "11625\n",
      "11650\n",
      "11675\n",
      "11700\n",
      "11725\n",
      "11750\n",
      "11775\n",
      "11800\n",
      "11825\n",
      "11850\n",
      "11875\n",
      "11900\n",
      "11925\n",
      "11950\n",
      "11975\n",
      "12000\n",
      "12025\n",
      "12050\n",
      "12075\n",
      "12100\n",
      "12125\n",
      "12150\n",
      "12175\n",
      "12200\n",
      "12225\n",
      "12250\n",
      "12275\n",
      "12300\n",
      "12325\n",
      "12350\n",
      "12375\n",
      "12400\n",
      "12425\n",
      "12450\n",
      "12475\n",
      "12500\n",
      "12525\n",
      "12550\n",
      "12575\n",
      "12600\n",
      "12625\n",
      "12650\n",
      "12675\n",
      "12700\n",
      "12725\n",
      "12750\n",
      "12775\n",
      "12800\n",
      "12825\n",
      "12850\n",
      "12875\n",
      "12900\n",
      "12925\n",
      "12950\n",
      "12975\n",
      "13000\n",
      "13025\n",
      "13050\n",
      "13075\n",
      "13100\n",
      "13125\n",
      "13150\n",
      "13175\n",
      "13200\n",
      "13225\n",
      "13250\n",
      "13275\n",
      "13300\n",
      "13325\n",
      "13350\n",
      "13375\n",
      "13400\n",
      "13425\n",
      "13450\n",
      "13475\n",
      "13500\n",
      "13525\n",
      "13550\n",
      "13575\n",
      "13600\n",
      "13625\n",
      "13650\n",
      "13675\n",
      "13700\n",
      "13725\n",
      "13750\n",
      "13775\n",
      "13800\n",
      "13825\n",
      "13850\n",
      "13875\n",
      "13900\n",
      "13925\n",
      "13950\n",
      "13975\n",
      "14000\n",
      "14025\n",
      "14050\n",
      "14075\n",
      "14100\n",
      "14125\n",
      "14150\n",
      "14175\n",
      "14200\n",
      "14225\n",
      "14250\n",
      "14275\n",
      "14300\n",
      "14325\n",
      "14350\n",
      "14375\n",
      "14400\n",
      "14425\n",
      "14450\n",
      "14475\n",
      "14500\n",
      "14525\n",
      "14550\n",
      "14575\n",
      "14600\n",
      "14625\n",
      "14650\n",
      "14675\n",
      "14700\n",
      "14725\n",
      "14750\n",
      "14775\n",
      "14800\n",
      "14825\n",
      "14850\n",
      "14875\n",
      "14900\n",
      "14925\n",
      "14950\n",
      "14975\n",
      "15000\n",
      "15025\n",
      "15050\n",
      "15075\n",
      "15100\n",
      "15125\n",
      "15150\n",
      "15175\n",
      "15200\n",
      "15225\n",
      "15250\n",
      "15275\n",
      "15300\n",
      "15325\n",
      "15350\n",
      "15375\n",
      "15400\n",
      "15425\n",
      "15450\n",
      "15475\n",
      "15500\n",
      "15525\n",
      "15550\n",
      "15575\n",
      "15600\n",
      "15625\n",
      "15650\n",
      "15675\n",
      "15700\n",
      "15725\n",
      "15750\n",
      "15775\n",
      "15800\n",
      "15825\n",
      "15850\n",
      "15875\n",
      "15900\n",
      "15925\n",
      "15950\n",
      "15975\n",
      "16000\n",
      "16025\n",
      "16050\n",
      "16075\n",
      "16100\n",
      "16125\n",
      "16150\n",
      "16175\n",
      "16200\n",
      "16225\n",
      "16250\n",
      "16275\n",
      "16300\n",
      "16325\n",
      "16350\n",
      "16375\n",
      "16400\n",
      "16425\n",
      "16450\n",
      "16475\n",
      "16500\n",
      "16525\n",
      "16550\n",
      "16575\n",
      "16600\n",
      "16625\n",
      "16650\n",
      "16675\n",
      "16700\n",
      "16725\n",
      "16750\n",
      "16775\n",
      "16800\n",
      "16825\n",
      "16850\n",
      "16875\n",
      "16900\n",
      "16925\n",
      "16950\n",
      "16975\n",
      "17000\n",
      "17025\n",
      "17050\n",
      "17075\n",
      "17100\n",
      "17125\n",
      "17150\n",
      "17175\n",
      "17200\n",
      "17225\n",
      "17250\n",
      "17275\n",
      "17300\n",
      "17325\n",
      "17350\n",
      "17375\n",
      "17400\n",
      "17425\n",
      "17450\n",
      "17475\n",
      "17500\n",
      "17525\n",
      "17550\n",
      "17575\n",
      "17600\n",
      "17625\n",
      "17650\n",
      "17675\n",
      "17700\n",
      "17725\n",
      "17750\n",
      "17775\n",
      "17800\n",
      "17825\n",
      "17850\n",
      "17875\n",
      "17900\n",
      "17925\n",
      "17950\n",
      "17975\n",
      "18000\n",
      "18025\n",
      "18050\n",
      "18075\n",
      "18100\n",
      "18125\n",
      "18150\n",
      "18175\n",
      "18200\n",
      "18225\n",
      "18250\n",
      "18275\n",
      "18300\n",
      "18325\n",
      "18350\n",
      "18375\n",
      "18400\n",
      "18425\n",
      "18450\n",
      "18475\n",
      "18500\n",
      "18525\n",
      "18550\n",
      "18575\n",
      "18600\n",
      "18625\n",
      "18650\n",
      "18675\n",
      "18700\n",
      "18725\n",
      "18750\n",
      "18775\n",
      "18800\n",
      "18825\n",
      "18850\n",
      "18875\n",
      "18900\n",
      "18925\n",
      "18950\n",
      "18975\n",
      "19000\n",
      "19025\n",
      "19050\n",
      "19075\n",
      "19100\n",
      "19125\n",
      "19150\n",
      "19175\n",
      "19200\n",
      "19225\n",
      "19250\n",
      "19275\n",
      "19300\n",
      "19325\n",
      "19350\n",
      "19375\n",
      "19400\n",
      "19425\n",
      "19450\n",
      "19475\n",
      "19500\n",
      "19525\n",
      "19550\n",
      "19575\n",
      "19600\n",
      "19625\n",
      "19650\n",
      "19675\n",
      "19700\n",
      "19725\n",
      "19750\n",
      "19775\n",
      "19800\n",
      "19825\n",
      "19850\n",
      "19875\n",
      "19900\n",
      "19925\n",
      "19950\n",
      "19975\n"
     ]
    }
   ],
   "source": [
    "#Webscraping\n",
    "#import pandas module\n",
    "import pandas as pd\n",
    "\n",
    " \n",
    "#initial value of x\n",
    "x = 0\n",
    "#To limit the number of loops\n",
    "limit = 20000\n",
    "#incremented every time by 25 values\n",
    "increment = 25\n",
    "#name of link\n",
    "linkbase = \"https://ustc.ac.uk/index.php/search/cicero?tm_fulltext=&tm_field_allauthr=&tm_translator=&tm_editor=&ts_field_short_title=&tm_field_imprint=&tm_field_place=&sm_field_year=&f_sm_field_year=&t_sm_field_year=&sm_field_country=&sm_field_lang=&sm_field_format=&sm_field_digital=&sm_field_class=%22Science+and+Mathematics%22&tm_field_cit_name=&tm_field_cit_no=&order=year_asc&sm_field_ty=true&start=0\"\n",
    "\n",
    "#while loop\n",
    "while (x < limit):\n",
    "#changing webpage\n",
    "    link=linkbase+str(x)\n",
    "#fitting the data to a table\n",
    "    tables = pd.read_html(link)\n",
    "\n",
    "    #print(tables[0])\n",
    "#converting the tables to a csv file\n",
    "    tables[0].to_csv(\"/Users/Awshesh/Desktop/Science1.csv\", mode='a', encoding='utf-8', index=False)\n",
    "\n",
    "    print(x)\n",
    "#incrementation of value of x by 1 after every loop\n",
    "    x += increment  \n",
    "    \n",
    "#Time taken-10-11 hours  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So , after receiving the datasets some of the books are not categorised for us and that too is a large number of books.So we need to categorise those books.The most basic approach is by natural language processing.That is -\n",
    "* Translate the text in English\n",
    "* Tokenize the text and clean the text by removing stopwords and punctuation\n",
    "* Lemmatization and stemming of text\n",
    "* Intersection of the text by our desired words corpus prepared by cleaning the text of known titles.\n",
    "\n",
    "Its a basic approach to understand the natural language processing tools and how to apply it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translation and categorisation\n",
    "#import module\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "filename=input(\"Please enter the name of a new file in which you want to save the data\")\n",
    "import nltk\n",
    "from pandas import ExcelWriter\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize ,sent_tokenize\n",
    "from nltk.stem import PorterStemmer , WordNetLemmatizer\n",
    "#Tokenization,Lemmatization and Stemming of text.\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "ps= PorterStemmer()\n",
    "l = 'scientific'\n",
    "k = 'non-scientific'\n",
    "#Corpus of science words\n",
    "science_words = set(['interpretation','aristotle','physical','toxic','smooth','cellular','rear','resistant','invasive','fungal','orbital','infected','microbial','optimum','microscopic','angular','acoustic','lightweight','biochemical','moist','adjustable','fluorescent','anaerobic','perpendicular','radial','metallic','treated','kinetic','boiling','bulky','cylindrical','fiber-optic','high-frequency','veterinary','acidic','powered','computer-aided','unaided','pneumatic','rotational','transverse','conductive','activated','soluble','coronal','mini','salty','dashed','alkaline','low-frequency','tubular','oxidative','granular','circadian','stainless-steel','shredded','waterproof','steady-state','microbiological','aromatic','dorsal','aqueous','digitized','inertial','tensile','adhesive','inflatable','bovine','nontoxic','accessory','electromechanical','overlying','patchy','airtight','chilled','molded','signal-to-noise','asymptotic','perforated','Gaussian','canine','nitric','snug','implantable','annular','calibrated','volumetric','flush','cell','laboratory','heat','protein','pound','egg','acid','load','bacteria','virus','tip','radiation','tube','filter','laser','angle','fluid','nutrient','oxygen','interface','imaging','velocity','valve','pulse','pesticide','membrane','specimen','enzyme','blade','fungus','x-ray','repair','compression','joint','kit','node','pathogen','moisture','scan','shower','pad','absorption','groundwater','plasma','liquid','cavity','vector','handle','heating','mutation','receptor','width','microwave','parasite','contaminant','mercury','thickness','mold','drainage','toxin','lift','microorganism','amino','radius','pollen','magnification','wastewater','mesh','mushroom','effluent','propulsion','poultry','spore','gradient','amplitude','vent','foam','zinc','wastewater','compartment','phosphorus','fitting','hood','gel','humidity','lobe','calibration','tick','saturation','ammonia','digestion','transducer','liter','traction','sponge','seafood','particulate','aquifer','liner','sigma','piston','scanning','info','amplifier','cycling','peptide','deformation','chloride','yeast','monoxide','pathologist','filtration','tilt','mole','microscopy','allele','pore','uptake','tubing','aerosol','packing','retina','assay','titanium','physiologist','nylon','faucet','adhesion','oxidation','disinfection','axle','pedal','respiration','vertebra','deflection','PDA','dryer','starch','spindle','additive','carbonate','uterus','airflow','casing','quad','drying','chromium','fluorescence','suction','phenotype','blueberry','melon','styling','beet','cam','electrolyte','sampler','genotype','irradiation','waveform','conduction','sulfate','ovary','pulley','cadmium','theta','vial','hemoglobin','mitochondria','polyethylene','legume','silicone','sled','batter','half-life','cellulose','feces','headset','chlorophyll','nucleotide','mum','placenta','impedance','mixer','tremor','ejection','polymerase','repeatability','drip','decompression','sedimentation','germination','bending','scallop','granule','flush','pacemaker','alkaloid','iodine','absorber','downtime','polyester','viscosity','ammonium','salmonella','spacer','acidity','papaya','caliper','dishwasher','microarray','yolk','toner','turbidity','abrasion','polypropylene','taper','thistle','flex','nitrite','argon','p-value','tamoxifen','polymorphism','chlorination','deceleration','microgram','acidification','cantaloupe','extrusion','riser','ligand','manganese','modulator','caster','kiwi','virulence','boron','nitrification','cation','regrowth','orifice','wick','seepage','tetracycline','hypochlorite','spectrometry','manually','preferentially','sh','rotate','warm','heat','image','spray','coat','digest','interface','secrete','clog','leach','incubate','bulk','thaw','excrete','inoculate','oxidize','lubricate','inactivate','high-end','biotech','pricey','nifty','flat-panel','receivable','start-up','automaker','mainframe','rig','minivan','electric','pager','fuselage','copier','halogen','outback','roofing','break-even','carmaker','retail','automate','digital','bright','solar','genetic','ecological','chemical','mechanical','electric','marine','magnetic','evolutionary','optical','electrical','tropical','molecular','cosmic','liquid','planetary','astronomical','atmospheric','infrared','faint','atomic','milky','dense','lunar','thermal','portable','gravitational','amateur','Arctic','aquatic','terrestrial','polar','composting','stellar','compact','celestial','binary','renewable','built-in','ultraviolet','spiral','three-dimensional','geological','metric','automated','freshwater','high-speed','volcanic','electromagnetic','rocky','nonlinear','computational','spectral','suicidal','automotive','Martian','civil-military','radioactive','dim','robotic','hydraulic','tidal','galactic','extinct','climatic','interstellar','equatorial','x-ray','globular','real-time','temperate','two-dimensional','cosmological','recycled','flash','freezing','Antarctic','stainless','elliptical','underwater','seismic','compressed','spotted','mating','curved','oceanic','luminous','high-resolution','migratory','macro','high-energy','switching','graphical','icy','spherical','high-performance','diagonal','ground-based','versatile','engineered','gamma-ray','stored','floppy','inorganic','modular','elongated','taxonomic','deep-sky','phylogenetic','riparian','unmanned','floating-point','extraterrestrial','larval','parasitic','algal','radiant','charged','rotary','Newtonian','grazing','mammalian','flowering','postdoctoral','vertebrate','feeding','gaseous','anthropogenic','on-screen','biotic','monochrome','forested','rotating','avian','meteorological','space-time','geologic','stand-alone','edible','pelagic','vegetative','woody','star','superconducting','inshore','condensed','switched','branching','telescopic','cooling','molten','catalytic','trophic','manned','photovoltaic','recursive','mitochondrial','aerodynamic','geothermal','photosynthetic','astrophysical','ultrasonic','full-size','crystalline','phenotypic','cloudy','glacial','programmable','fractional','removable','benthic','transgenic','dissolved','reddish','close-up','epoxy','genomic','high-pressure','fractal','introduced','long-lived','nesting','supersonic','deep-sea','macroscopic','reusable','frigid','deep-water','amphibian','shaded','above-ground','habitable','hind','extrasolar','symmetric','heavy-duty','geophysical','next-generation','offset','plug-in','submersible','energy-efficient','thin-film','cordless','binocular','easy-to-use','invertebrate','patented','recyclable','tectonic','corrugated','high-definition','subtropical','naked-eye','thermonuclear','interplanetary','high-temperature','Bayesian','high-density','orbiting','front-end','spectroscopic','subatomic','coal-fired','biophysical','Magellanic','insulating','ionized','biodegradable','undersea','off-line','ecliptic','Himalayan','hydrologic','waste','viscous','burst','eroded','horticultural','object-oriented','intergalactic','abrasive','hydrological','incandescent','on-board','flattened','high-altitude','jovian','spinning','low-power','old-growth','solid-state','roomy','planar','Pleistocene','zodiacal','off-road','trapped','bundled','outermost','sulfuric','dynamical','sedimentary','boreal','biometric','cometary','deciduous','oceanographic','off-the-shelf','pressurized','electrostatic','omitted','fast-moving','dielectric','electrochemical','rechargeable','hominid','nitrous','coupled','holographic','wide-angle','galvanized','hexagonal','radiative','science-fiction','sea-level','sporty','photonic','blue-green','cosmic-ray','near-infrared','stereoscopic','estuarine','high-power','bluish','earthbound','grating','open-source','belowground','fossilized','blurry','carnivorous','earth-like','low-energy','non-destructive','all-sky','decimal','general-purpose','rounding','pre-dawn','brushless','color-coded','cryogenic','inelastic','thermodynamic','wide-screen','computer-controlled','near-earth','hydrothermal','spiny','stochastic','sun-like','topological','Dobsonian','intertidal','U-shaped','saltwater','low-pressure','unobstructed','asexual','compostable','piezoelectric','upgraded','backlit','battery-powered','lithium-ion','yellowish','Cretaceous','oversize','acrylic','nonformal','featureless','open-access','abiotic','low-mass','serpentine','closed-loop','all-new','Galilean','greenish','pro-environmental','V-shaped','star','species','plant','scientist','surface','earth','software','forest','sun','fish','planet','temperature','telescope','soil','engineering','camera','fuel','speed','universe','galaxy','sky','file','engineer','gene','emission','drive','engine','moon','PC','bird','hole','astronomer','conservation','atmosphere','wind','layer','carbon','ice','physics','cluster','ocean','lab','chip','particle','stream','magnitude','disk','habitat','cloud','density','storage','ecosystem','satellite','fishing','DNA','battery','organism','leaf','meter','lens','biology','fishery','dust','astronomy','ring','insect','molecule','electron','orbit','observatory','hardware','atom','slide','comet','simulation','pump','mouse','sensor','gravity','mount','hydrogen','compost','processor','gear','nitrogen','ecology','pipe','fossil','abundance','diameter','quantum','wildlife','salmon','clock','beam','jet','seal','physicist','mechanic','biodiversity','button','printer','biologist','ray','dioxide','algorithm','catch','nucleus','whale','courtesy','module','landing','graphic','vacuum','mammal','nebula','oyster','asteroid','sediment','greenhouse','tail','detector','chemistry','switch','electronics','patch','robot','predator','crystal','recycling','prototype','fly','desktop','shark','fisherman','server','wavelength','spacecraft','silicon','vegetation','turbine','eclipse','aluminum','prey','nest','wireless','dot','rocket','bee','genome','constellation','debris','launch','laptop','controller','disc','breeding','notebook','cache','rail','explorer','airplane','warming','shrimp','odor','loop','collision','crater','ant','brightness','fax','tunnel','landfill','fluctuation','meteor','bat','pond','folder','reactor','computing','extinction','lighting','watershed','supernova','turtle','tag','sunlight','coral','fertilizer','reef','lobster','photon','stereo','latitude','setup','grower','burst','voltage','radar','larva','shuttle','altitude','pest','pixel','biomass','crab','update','neutron','ink','buffer','optics','eyepiece','dwarf','volcano','worm','light-year','geometry','spin','slot','vibration','genetics','embryo','flux','auditor','ethanol','neutrino','sunset','bearing','screw','gamma','cylinder','feed','frog','ion','bang','sperm','polymer','chlorine','spider','butterfly','residuals','backyard','weed','ram','torque','browser','combustion','diesel','chromosome','biosolids','rainfall','antenna','semiconductor','dome','analog','brake','genus','cad','coating','runoff','dinosaur','trimming','shaft','tuna','microbe','oxide','aerial','microscope','symmetry','sink','chemist','astronaut','canopy','composting','algae','hybrid','meteorite','beetle','precipitation','inlet','relativity','scanner','upgrade','owl','sulfur','bolt','amateur','zoo','ecologist','tab','solid','swamp','vapor','glow','substrate','RNA','proton','coupling','crust','binoculars','glue','marsh','aerospace','bandwidth','batch','aperture','euro','app','password','CPU','audio','subsystem','cartridge','exhaust','reflector','geologist','centimeter','tray','fox','dolphin','constant','methane','rodent','wiring','fabrication','manure','oscillation','spreadsheet','microprocessor','halo','pulsar','moth','primate','tripod','lander','helium','mantle','steering','quasar','lizard','drawer','adapter','transistor','cadet','lava','connector','optimization','twilight','atlas','trawl','exhibitor','plume','sighting','daylight','refractor','glacier','sunrise','mathematician','saw','tornado','plug','transmitter','windrow','alloy','beluga','hydrocarbon','binary','shrub','fauna','predation','litter','penguin','actuator','hose','wasp','UFO','diver','router','geology','aquaculture','equator','hull','dispersal','propagation','rockfish','coil','download','boiler','turbulence','termite','rotor','geneticist','grassland','automation','macro','rainforest','chick','corrosion','burrow','chimpanzee','dialog','finder','crescent','accelerator','flare','shutter','estuary','isotope','sedan','resin','millimeter','theorem','insulation','squid','decomposition','organic','simulator','snail','warranty','clutch','sensing','sub','omega','plywood','shortcut','compressor','gorilla','reptile','trout','feedstock','entomologist','chassis','astrophysicist','epsilon','fern','furnace','tropics','camcorder','chimp','bloom','burner','clone','heater','squirrel','functionality','tau','caterpillar','quark','redshift','aquarium','seedling','clump','bait','mite','woodland','rover','breeder','excitation','angler','biota','carcass','elk','zoom','filament','quake','groove','knob','fertilization','cub','evaporation','auditing','astrophysics','peat','crane','converter','spacing','adobe','pumping','shellfish','glare','clam','paleontologist','cooling','diffraction','mulch','haze','juvenile','venom','accretion','vertebrate','amphibian','fingerprint','coma','propeller','spam','iteration','invertebrate','bulge','piping','exponent','neon','lubricant','salinity','spectroscopy','analyzer','insecticide','photosynthesis','connectivity','hardwood','queue','rust','wafer','payload','luminosity','convection','IPod','kernel','urchin','droplet','aspen','lattice','seawater','weevil','cardboard','spectrometer','herbicide','micron','robotics','sunspot','scattering','locomotive','encryption','vole','bison','welding','feeder','conveyor','longitude','hatchery','tortoise','throughput','condensate','corona','iPhone','planetarium','bushel','cactus','circuitry','firewall','lichen','nozzle','throttle','asphalt','carnivore','headphone','occultation','pod','clamp','botanist','vortex','savanna','millisecond','swarm','mussels','cockpit','inbreeding','cosmologist','grinder','lithium','seeing','hybridization','perturbation','photojournalism','focuser','orchid','stratosphere','fission','inset','mutant','spec','trough','fastener','horsepower','mangrove','takeoff','biofuel','compiler','malware','modulation','stud','baboon','tracer','elm','ignition','supercomputer','entropy','nectar','thunderstorm','cellphone','flashlight','pane','moose','ref','fireball','foliage','nematode','swap','motherboard','quarantine','exchanger','hyena','zebra','diode','rangeland','turbo','mating','otter','refrigeration','spruce','toolbar','appendage','nanotube','orbiter','diagnostics','longline','washer','reservist','digester','declination','germplasm','sugarcane','lick','telltale','tug','infestation','pellet','sequestration','beaver','hauler','conductivity','microbiologist','phytoplankton','pollinator','sonar','underside','dew','fir','giraffe','blower','wrench','condensation','lithography','curvature','eel','ascension','cormorant','nanometer','servo','mackerel','snapper','elongation','qubit','twig','interferometer','rhino','stalk','condor','encoder','plating','oval','magnetism','tundra','yellowfin','aeration','polytechnic','wildflower','herbivore','beak','blight','botany','curbside','micrometer','adhesive','ecoregion','hominid','silt','catchment','forage','decoder','technologist','projectile','hatch','judging','alfalfa','console','cursor','toolbox','pathfinder','integer','siding','bore','finch','gill','transformer','blur','moss','brood','darkroom','peripheral','seafloor','blob','latch','subspecies','aphid','charger','dredging','swine','biofilter','topsoil','drone','fecundity','topology','enlistment','glitch','plankton','graphite','slider','acorn','tern','coupe','genomics','titan','muon','windshield','equinox','loader','recyclable','salamander','spectrograph','fermentation','pollination','slug','cathode','fiberglass','sharpness','flange','magma','slit','starlight','pop-up','postdoc','toad','boll','catfish','airbag','methyl','perch','positron','sawdust','chrome','frost','flyby','octopus','shareware','sulfide','lubrication','silica','aurora','biodiesel','meteorologist','mower','scorpion','tuber','header','coolant','spaceship','nebulosity','permafrost','synchronization','synchrotron','refrigerant','outage','maggot','capacitance','kill','renewables','headlight','partitioning','propellant','birch','screwdriver','scrubber','swordfish','rafter','arcsecond','cantilever','glider','arthropod','hummingbird','megawatt','grasshopper','gull','rocker','android','antimatter','slurry','quartz','songbird','sphinx','aeronautics','tuner','browsing','desalination','dipper','lightweight','thermodynamics','handset','tadpole','wildfire','blocking','windmill','conservancy','joystick','operand','pallet','conch','spyware','cobalt','collider','manifold','metadata','propane','dredge','saucer','brine','sorting','dissipation','arboretum','roughness','skimmer','thumbnail','URL','buoy','tenon','trawler','amp','drywall','eutrophication','lepton','oscillator','cyanide','tabletop','cyclone','detonation','joist','perihelion','radioactivity','dashboard','bot','ethernet','readout','emitter','chisel','flooring','hopper','mollusk','woodpecker','wavefunction','firefly','monsoon','seabird','skunk','conifer','gasification','timescale','capacitor','sorghum','superconductor','astrophotography','detritus','exoplanet','crustacean','discoverer','gearbox','geyser','mounting','stormwater','weasel','pheromone','viewfinder','zooplankton','aerodynamics','alligator','mortise','sprinkler','hydrology','raptor','yo-yo','cogeneration','gypsum','reforestation','snowfall','ionization','meteoroid','bookmark','entomology','vane','wormhole','oceanography','buoyancy','honeybee','nonlinearity','routing','tonnage','billfish','molding','recombination','tillage','invariance','taxonomist','archeologist','coloration','gravitation','monochrome','radiator','self-organization','stork','angiosperm','auger','precession','taskbar','magnetar','accelerometer','histogram','ellipse','emulsion','fungicide','hadron','jellyfish','blaster','dram','orangutan','tungsten','helix','keypad','pelican','usability','modeler','paleontology','polyurethane','snowflake','weathering','buckyball','cleat','cyclotron','wildebeest','zoology','afterglow','deuterium','in-box','retrofit','skiff','slat','sparrow','wimp','beekeeper','flywheel','gigabyte','soot','workhorse','bioscience','nucleon','speciation','tweak','dynamo','gust','miter','tectonics','viper','bioremediation','gametophyte','snout','trilobite','azimuth','cheetah','cicada','exciton','lemma','portable','surround','telephoto','understory','composter','dataflow','grinding','overdrive','twitter','wavefront','microelectronics','reflectance','adder','impoundment','prawn','transect','copulation','dipole','hangar','rake','rattlesnake','sauropod','zoologist','biochemist','organelle','vise','biocontrol','desiccation','isomer','meteorology','ornithologist','polynomial','skew','terminator','anchovy','edit','superconductivity','dowel','drongo','dryland','morph','orb','reducer','kilowatt-hour','magenta','magnetosphere','micrograph','phylogeny','telomere','anisotropy','sapling','webcam','airship','condenser','iota','superposition','airfoil','gallium','hibernation','seagrass','separator','wheelbase','landscaper','methylmercury','baffle','beehive','coho','earthworm','truss','petri','photosphere','planetesimal','resonator','shifter','volcanism','firmware','grille','bootstrap','clipboard','critter','damper','dick','flipper','iguana','overheating','runner-up','wisp','attractor','diffuser','ling','periodicity','recapture','collimation','e-book','heuristics','prion','woodworking','environmentally','genetically','experimentally','chemically','electrically','digitally','underwater','seasonally','gravitationally','skyward','optically','install','photograph','click','cool','scan','tire','slide','harvest','emit','recycle','shine','download','simulate','power','tag','plug','optimize','orbit','compost','fish','space','compress','machine','screw','collide','glow','radiate','configure','forage','stream','graze','mate','clamp','hatch','format','reuse','nest','browse','glue','soil','fasten','decay','zoom','brighten','thread','eject','dim','reset','synchronize','darken','tow','tweak','recharge','upload','fertilize','oscillate','slot','spew','brake','decompose','burrow','ionize','weld','trawl','output','tunnel','latch','sputter','germinate','warp','scroll','manure','vaporize','clock','retrofit','infest','bait','troubleshoot','encrypt','occult','tool','transit','pivot','slew','accrete','backup','winter','sand','sequester','roost','outshine','incinerate','pollinate','queue','swivel','uncheck','fertilized','Copernican','cosmos','pup','creationism','systematics','abductee','life-form','heritability','tube','unit','vocabulary','water','weight','cuboid','angle','addition','activity','decimal','factor','cube','aim','denominator','fraction','digit','application','integers','prime','division','area','numerator','estimate','cost','parallelogram','mass','perimeter','multiplication','protractor','number','faces','simulator','percentage','figure','trapezium','rectangle','vertices','square','height','subtraction','hour','sum','length','tonnes','line','triangle','minutes','volume','months','zero','page','plan','problem','procedure','process','question','sides','solution','system','temperature','term','terms','unit','value','balance','angled','copy','change','happen','calculate','determine','effect','carry','draw','float','learn','convert','find','flow','need','devise','follow','measure','show','divided','increasing','observe','understand','evaluate','involving','record','measure','is','multiply','mixed','perform','represent','shaded','try','simplify','understand','solve','renewable','total','available','straight','soluble','multiple','scientific','adjacent','wooden','common','natural','parallel','basic','parallel','potential','acute','kinetic','various','obtuse','convestion','unknown','blue','improper','red','algebraic','dull','moist','hot','measuring','living','tube','unit','vocabulary','water','weight','cuboid','angle','addition','activity','decimal','factor','cube','aim','denominator','fraction','digit','application','integers','prime','division','area','numerator','estimate','cost','parallelogram','mass','perimeter''multiplication','protractor','number','faces','simulator','percentage','figure','trapezium','rectangle','vertices','square','height','subtraction','hour','sum','length','tonnes','line','triangle','minutes','volume','months','zero','page','plan','problem','procedure','process','question','sides','solution','system','temperature','term','terms','unit','value','balance','angled','copy','change','happen','calculate','determine','effect','carry','draw','float','learn','convert','find','flow','need','devise','follow','measure','show','divided','increasing','observe','understand','evaluate','involving','record','measure','is','multiply','mixed','perform','represent','shaded','try','simplify','understand','solve','renewable','total','available','straight','soluble','multiple','scientific','adjacent','wooden','common','natural','parallel','basic','parallel','potential','acute','kinetic','various','obtuse','convestion','unknown','blue','improper','red','algebraic','dull','moist','hot','measuring','living','Bunsen','burner','cell','apparatus','activity','protoplasm','combustion','beaker','air','hydroxide','composition','bicarbonate','mical','body','sodium','conical','candle','copper','colour','cylinder','conclusion','density','earth','experiment','elements','filter','energy','fuels','gas','figure','gauze','flame','kerosene','flask','laboratory','function','lever','glass','mass','heat','matter','hot','mercury','human','metal','ice','microorganisms','indicator','microscope','input','minerals','investigation','nitrogen','jar','organisms','length','oxygen','particles','life','pendulum','light','radian','lime','respiration','liquid','rod','material','rubber','measurement','science','method','space','metre','substance','mixture','sulphur','movement','thermometer','object','vapour','observation','variable','option','volume','outcomes','paper','photophysical','plant','procedure','process','properties','quantity','red','report','resources','salt','soil','solid','solution','sources','splinter','structure','sun','surface','system','temperature','test','things'])\n",
    "\n",
    "# Creation of function\n",
    "def genderize(sentence):\n",
    "    words = word_tokenize(sentence)        \n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    lem = [lemmatiser.lemmatize(t) for t in words]\n",
    "    print(lem)\n",
    "    length =len(science_words.intersection(lem))\n",
    "    if length>0:\n",
    "                   return l\n",
    "    else:\n",
    "                   return k\n",
    "#import csv module                \n",
    "import csv\n",
    "#Textblob library for translation of text\n",
    "from textblob import TextBlob\n",
    "with open('ustcmasterfullyearschanged.csv',encoding='Latin-1') as csvDatafile:\n",
    "    data=list(csv.reader(csvDatafile))\n",
    "    i=129\n",
    "    gy=[]\n",
    "    oi=[]\n",
    "    while(i<2926):\n",
    "        wenja=(data[i][1])\n",
    "        #print(wenja)\n",
    "        sentence = TextBlob(str(wenja))\n",
    "        op=sentence.detect_language()\n",
    "        b=str(op)\n",
    "        z=sentence.translate(from_lang= b , to ='en')\n",
    "        q=str(z).lower()\n",
    "        print(q)\n",
    "        gy.append(q)\n",
    "        oi.append(genderize(q))\n",
    "        \n",
    "        i+=1\n",
    "        print(gy)\n",
    "        print(oi)\n",
    "df_result=pd.DataFrame(list(zip(gy,oi)),columns=['Title','Category'])\n",
    "df_result.to_csv(str(filename)+('.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Machine learning using Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this,we will be covering the steps on how to do Latent Dirichlet Allocation (LDA), which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide\n",
    "* A document-term matrix \n",
    "* The number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic Modelling using Latent Dirichlet Allocation\n",
    "import pandas as pd\n",
    "#For reading the dataset\n",
    "filename=input(Please enter the name of the file on which you want to apply topic modelling)\n",
    "data = pd.read_csv((str(filename)+'.csv'), error_bad_lines=False,encoding=\"LATIN-1\")\n",
    "#For selecting the column of a specific dataset.\n",
    "data_text = data[['Title']]\n",
    "data_text['index'] =data_text.index\n",
    "documents = data_text\n",
    "print(len(documents))\n",
    "print(documents[:5])\n",
    "#Gensim library\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer=SnowballStemmer('english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    a=WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "    list1=[]\n",
    "    list1.append(a)\n",
    "    for each in list1:\n",
    "        ki=stemmer.stem(each)\n",
    "    return ki\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "doc_sample = documents[documents['index'] == 2].values[0][0]\n",
    "##print('original document: ')\n",
    "words = []\n",
    "words.append(word_tokenize(doc_sample))\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))\n",
    "#lets try for another 10 titles\n",
    "processed_docs = documents['title'].map(preprocess(documents[]))\n",
    "z=processed_docs[:10]\n",
    "print(z)\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "print(count)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[10]\n",
    "bow_doc_10 = bow_corpus[10]\n",
    "for i in range(len(bow_doc_10)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_10[i][0], \n",
    "                                               dictionary[bow_doc_10[i][0]], \n",
    "bow_doc_10[i][1]))\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=4, id2word=dictionary, passes=2, workers=2)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output example- ('0.005*\"medicine\" + 0.005*\"sports\" + 0.005*\"test tube\" + 0.004*\"bandages\")\n",
    "* Topic modelling is quite time consuming.eg.for 50 lines it took approximately 3 hours for the execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets consider we want to classify into the categories we already have in our dataset. So, to obtain similar categories we shall first try to train our model accordingly and fit the data we already know and later try to predict similar data by our training model.\n",
    "\n",
    "Types of Algorithms we used-\n",
    "* Linear Regression\n",
    "* Logistic Regression\n",
    "* Naive Bayes\n",
    "* Support Vector Machines\n",
    "\n",
    "\n",
    "Different Algorithms have different accuracies which also depends on our trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for Linear Regression\n",
    "#import module\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from pandas import ExcelWriter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.svm import SVC,LinearSVC,NuSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "filename1=input(\"Please enter the name of file in which we have titles categorised as useful or non useful\")\n",
    "filename2=input(\"Please enter the name of file in which we have titles uncategorised\")\n",
    "filename=input(\"Please enter the name of file in which you want to save the results\")\n",
    "#append the data to a dataframe\n",
    "df= pd.read_csv((str(filename1)+\".csv\"),encoding=\"LATIN-1\",low_memory=False)\n",
    "df_x=df[\"Title\"]\n",
    "df_y=df[\"Useful\"]\n",
    "#Vectorize the text to fit in the space\n",
    "cv=TfidfVectorizer(min_df=1)\n",
    "x_train ,x_test,y_train,y_test = train_test_split(df_x,df_y,test_size=0.2)\n",
    "x_traincv = cv.fit_transform(x_train)\n",
    "mnb=LinearRegression()\n",
    "y_train=y_train.astype('int')\n",
    "mnb.fit(x_traincv,y_train)\n",
    "x_testcv=cv.transform(x_test)\n",
    "pred=mnb.predict(x_testcv)\n",
    "actual=np.array(y_test)\n",
    "count = 0\n",
    "count1=0\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] ==1:\n",
    "        if actual[i]==1:            \n",
    "            count=count+1\n",
    "for i in range(len(pred)):\n",
    "    if pred[i]==actual[i]:\n",
    "        count1=count1+1\n",
    "print(count1)\n",
    "print(len(pred))\n",
    "print(actual)\n",
    "tn,fp,fn,tp=confusion_matrix(actual,pred).ravel()\n",
    "print(tn,fp,fn,tp)\n",
    "gy=[]\n",
    "oi=[]\n",
    "print(x_test)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "for each in x_test:\n",
    "    gy.append(each)\n",
    "for op in y_test:\n",
    "   oi.append(op)\n",
    "# import csv module\n",
    "import csv\n",
    "df2=pd.read_csv((str(filename2)+\".csv\"),encoding=\"LATIN-1\",low_memory = False)\n",
    "df1_x=df2[\"Title\"]\n",
    "df1_y=df2[\"Useful\"]\n",
    "print(len(df1_x))\n",
    "print(df1_y)\n",
    "record_list = [ list(item) for item in list(zip(x_test,y_test)) ]\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(df1_x,df1_y,test_size=0.999999999999)\n",
    "print(X_test)\n",
    "print(len(X_test))\n",
    "X_testcv=cv.transform(X_test)\n",
    "pred2=mnb.predict(X_testcv)\n",
    "print(pred2)\n",
    "print(len(pred2))\n",
    "df_result=pd.DataFrame(list(zip(X_test,pred2)),columns=['Title','Useful'])\n",
    "df_result.to_csv(str(filename)+('.csv'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 18 fields in line 12376, saw 26\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f313f046710e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Testmasterfile.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'LATIN-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter the name of place of which you want to check the freshness\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m##df=pd.read_csv(\"ustcmasterfullyearschanged.csv\",low_memory=False,encoding='LATIN-1')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Awshesh\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Awshesh\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Awshesh\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Awshesh\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 18 fields in line 12376, saw 26\n"
     ]
    }
   ],
   "source": [
    "from pandas import ExcelWriter\n",
    "import numpy as np\n",
    "import nltk\n",
    "import statistics\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"Testmasterfile.csv\",low_memory=False,encoding='LATIN-1')\n",
    "place=input(\"Enter the name of place of which you want to check the freshness\")\n",
    "##df=pd.read_csv(\"ustcmasterfullyearschanged.csv\",low_memory=False,encoding='LATIN-1')\n",
    "po=place\n",
    "df_a=df['author']\n",
    "df_b=df['title']\n",
    "df_c=df['place']\n",
    "df_d=df['year']\n",
    "df_e=df['first_year']\n",
    "df1=pd.read_csv(\"stopwords.csv\", low_memory=False, encoding='LATIN-1')\n",
    "df1_a=df1['author']\n",
    "df1_b=df1['first_year']\n",
    "i=0\n",
    "aa=[]\n",
    "ab=[]\n",
    "ac=[]\n",
    "ad=[]\n",
    "ae=[]\n",
    "fr=[]\n",
    "fg=[]\n",
    "while(i<683395):\n",
    "    if po == str(df_c[i]):\n",
    "        aa.append(df_a[i])\n",
    "        ab.append(df_b[i])\n",
    "        ac.append(df_d[i])\n",
    "    i+=1\n",
    "ad=list(set(aa))\n",
    "ae=list(set(ac))\n",
    "ae.sort()\n",
    "print(ae)\n",
    "y=0\n",
    "while(y<(len(ae)-1)):\n",
    "    print(ae[y])\n",
    "    f=0\n",
    "    er=[]\n",
    "    while(f<(len(ac)-1)):\n",
    "        if ae[y]==ac[f]:\n",
    "            er.append(aa[f])\n",
    "        f+=1    \n",
    "    ef=list(set(er))\n",
    "    opo=0\n",
    "    while(opo<(len(ef)-1)):\n",
    "        if 'nan' == str(ef[opo]):\n",
    "            del ef[opo]\n",
    "        opo+=1    \n",
    "    print(ef)\n",
    "    a=0\n",
    "    agelis=[]\n",
    "    if len(ef)>0:\n",
    "        while(a<(len(ef)-1)):\n",
    "            b=0\n",
    "            while(b<(len(df_a)-1)):\n",
    "                if ef[a]==df_a[b]:\n",
    "                    age = int(ae[y])-int(df_e[b])\n",
    "                    agelis.append(age)\n",
    "                b+=1\n",
    "                print(agelis)\n",
    "            a+=1\n",
    "            \n",
    "    else:\n",
    "        agelis.append('nan')        \n",
    "##    ty=0\n",
    "##    agelis=[]\n",
    "##    if (len(ef)>0):\n",
    "##        while(ty<(len(ef)-1)):\n",
    "##            op=0\n",
    "##            while(op<(len(df1_a)-1)):\n",
    "##                if ef[ty]==df1_a[op]:\n",
    "##                    age=(int(ae[y])-int(df1_b[op]))\n",
    "##                    print(age)\n",
    "##                op+=1 \n",
    "##                agelis.append(age)\n",
    "##            print(agelis)    \n",
    "##        mean_age=statistics.mean(agelis)\n",
    "##        fg.append(mean_age)\n",
    "##        print(mean_age)\n",
    "            \n",
    "    y+=1    \n",
    "    fr.append(ae[y])\n",
    "import openpyxl    \n",
    "df_result=pd.DataFrame(list(zip(fr,fg)),columns=['Year','Mean_age'])\n",
    "df_result.to_csv(\"MinimumYear.cs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression didn't predict good results because it also produced values beyond the limits.It takes a lot of time for the prediction.Approx.11 hours for a dataset of 110 strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "#import module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.svm import SVC,LinearSVC,NuSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#append the data to a dataframe\n",
    "filename1=input(\"Please enter the name of file in which we have titles categorised as useful or non useful\")\n",
    "filename2=input(\"Please enter the name of file in which we have titles uncategorised\")\n",
    "filename=input(\"Please enter the name of file in which you want to save the results\")\n",
    "df= pd.read_csv((str(filename1)+\".csv\"),encoding=\"LATIN-1\",low_memory=False)\n",
    "df_x=df[\"Title\"]\n",
    "df_y=df[\"Useful\"]\n",
    "#Vectorize the text to fit in the space\n",
    "cv=TfidfVectorizer(min_df=1)\n",
    "x_train ,x_test,y_train,y_test = train_test_split(df_x,df_y,test_size=0.2)\n",
    "x_traincv = cv.fit_transform(x_train)\n",
    "\n",
    "mnb=LogisticRegression()\n",
    "y_train=y_train.astype('int')\n",
    "mnb.fit(x_traincv,y_train)\n",
    "x_testcv=cv.transform(x_test)\n",
    "pred=mnb.predict(x_testcv)\n",
    "actual=np.array(y_test)\n",
    "count = 0\n",
    "count1=0\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] ==1:\n",
    "        if actual[i]==1:            \n",
    "            count=count+1\n",
    "for i in range(len(pred)):\n",
    "    if pred[i]==actual[i]:\n",
    "        count1=count1+1\n",
    "print(count1)\n",
    "print(len(pred))\n",
    "print(actual)\n",
    "tn,fp,fn,tp=confusion_matrix(actual,pred).ravel()\n",
    "print(tn,fp,fn,tp)\n",
    "gy=[]\n",
    "oi=[]\n",
    "print(x_test)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "for each in x_test:\n",
    "    gy.append(each)\n",
    "for op in y_test:\n",
    "   oi.append(op)\n",
    "# import csv module\n",
    "import csv\n",
    "df2=pd.read_csv((str(filename2)+\".csv\"),encoding=\"LATIN-1\",low_memory = False)\n",
    "df1_x=df2[\"Title\"]\n",
    "df1_y=df2[\"Useful\"]\n",
    "print(len(df1_x))\n",
    "print(df1_y)\n",
    "record_list = [ list(item) for item in list(zip(x_test,y_test)) ]\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(df1_x,df1_y,test_size=0.999999999999)\n",
    "print(X_test)\n",
    "print(len(X_test))\n",
    "X_testcv=cv.transform(X_test)\n",
    "pred2=mnb.predict(X_testcv)\n",
    "print(pred2)\n",
    "print(len(pred2))\n",
    "df_result=pd.DataFrame(list(zip(X_test,pred2)),columns=['Title','Useful'])\n",
    "df_result.to_csv(str(filename)+('.csv'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again,we couldn't rely on the values of the logistic regression because we had the ratio of 1's to 0's as 12.8%. So, for robustness check we had approx.74% of our 1's out of all 1's to be correct. But, maybe all our values could be correct.\n",
    "* Time Taken for 1,43,000 datasets was approx.20 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "#import module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.svm import SVC,LinearSVC,NuSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#append the data to a dataframe\n",
    "filename1=input(\"Please enter the name of file in which we have titles categorised as useful or non useful\")\n",
    "filename2=input(\"Please enter the name of file in which we have titles uncategorised\")\n",
    "filename=input(\"Please enter the name of file in which you want to save the results\")\n",
    "df= pd.read_csv((str(filename1)+\".csv\"),encoding=\"LATIN-1\",low_memory=False)\n",
    "df_x=df[\"Title\"]\n",
    "df_y=df[\"Useful\"]\n",
    "#Vectorize the text to fit in the spcae\n",
    "cv=TfidfVectorizer(min_df=1)\n",
    "x_train ,x_test,y_train,y_test = train_test_split(df_x,df_y,test_size=0.2)\n",
    "x_traincv = cv.fit_transform(x_train)\n",
    "\n",
    "mnb=GaussianNB()\n",
    "y_train=y_train.astype('int')\n",
    "mnb.fit(x_traincv,y_train)\n",
    "x_testcv=cv.transform(x_test)\n",
    "pred=mnb.predict(x_testcv)\n",
    "actual=np.array(y_test)\n",
    "count = 0\n",
    "count1=0\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] ==1:\n",
    "        if actual[i]==1:            \n",
    "            count=count+1\n",
    "for i in range(len(pred)):\n",
    "    if pred[i]==actual[i]:\n",
    "        count1=count1+1\n",
    "print(count1)\n",
    "print(len(pred))\n",
    "print(actual)\n",
    "tn,fp,fn,tp=confusion_matrix(actual,pred).ravel()\n",
    "print(tn,fp,fn,tp)\n",
    "gy=[]\n",
    "oi=[]\n",
    "print(x_test)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "for each in x_test:\n",
    "    gy.append(each)\n",
    "for op in y_test:\n",
    "   oi.append(op)\n",
    "# import csv module\n",
    "import csv\n",
    "df2=pd.read_csv((str(filename2)+\".csv\"),encoding=\"LATIN-1\",low_memory = False)\n",
    "df1_x=df2[\"Title\"]\n",
    "df1_y=df2[\"Useful\"]\n",
    "print(len(df1_x))\n",
    "print(df1_y)\n",
    "record_list = [ list(item) for item in list(zip(x_test,y_test)) ]\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(df1_x,df1_y,test_size=0.999999999999)\n",
    "print(X_test)\n",
    "print(len(X_test))\n",
    "X_testcv=cv.transform(X_test)\n",
    "pred2=mnb.predict(X_testcv)\n",
    "print(pred2)\n",
    "print(len(pred2))\n",
    "df_result=pd.DataFrame(list(zip(X_test,pred2)),columns=['Title','Useful'])\n",
    "df_result.to_csv(str(filename)+('.csv'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We couldn't rely on the results of NaiveBayes Algorithm because of the same reason of accuracy of testing/training ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Classifiers\n",
    "#import module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from pandas import ExcelWriter\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.svm import SVC,LinearSVC,NuSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#append the data to a dataframe\n",
    "filename1=input(\"Please enter the name of file in which we have titles categorised as useful or non useful\")\n",
    "filename2=input(\"Please enter the name of file in which we have titles uncategorised\")\n",
    "filename=input(\"Please enter the name of file in which you want to save the results\")\n",
    "df= pd.read_csv((str(filename1)+\".csv\"),encoding=\"LATIN-1\",low_memory=False)\n",
    "df_x=df[\"Title\"]\n",
    "df_y=df[\"Useful\"]\n",
    "#Vectorize the text to fit in the spcae\n",
    "cv=TfidfVectorizer(min_df=1,stop_words='english')\n",
    "x_train ,x_test,y_train,y_test = train_test_split(df_x,df_y,test_size=0.2)\n",
    "x_traincv = cv.fit_transform(x_train)\n",
    "\n",
    "mnb=SVC()\n",
    "y_train=y_train.astype('int')\n",
    "mnb.fit(x_traincv,y_train)\n",
    "x_testcv=cv.transform(x_test)\n",
    "pred=mnb.predict(x_testcv)\n",
    "actual=np.array(y_test)\n",
    "count = 0\n",
    "count1=0\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] ==1:\n",
    "        if actual[i]==1:            \n",
    "            count=count+1\n",
    "for i in range(len(pred)):\n",
    "    if pred[i]==actual[i]:\n",
    "        count1=count1+1\n",
    "print(count1)\n",
    "print(len(pred))\n",
    "print(actual)\n",
    "tn,fp,fn,tp=confusion_matrix(actual,pred).ravel()\n",
    "print(tn,fp,fn,tp)\n",
    "gy=[]\n",
    "oi=[]\n",
    "print(x_test)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "for each in x_test:\n",
    "    gy.append(each)\n",
    "for op in y_test:\n",
    "   oi.append(op)\n",
    "# import csv module\n",
    "import csv\n",
    "df2=pd.read_csv((str(filename2)+\".csv\"),encoding=\"LATIN-1\",low_memory = False)\n",
    "df1_x=df2[\"Title\"]\n",
    "df1_y=df2[\"Useful\"]\n",
    "print(len(df1_x))\n",
    "print(df1_y)\n",
    "record_list = [ list(item) for item in list(zip(x_test,y_test)) ]\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(df1_x,df1_y,test_size=0.999999999999)\n",
    "print(X_test)\n",
    "print(len(X_test))\n",
    "X_testcv=cv.transform(X_test)\n",
    "pred2=mnb.predict(X_testcv)\n",
    "print(pred2)\n",
    "print(len(pred2))\n",
    "df_result=pd.DataFrame(list(zip(X_test,pred2)),columns=['Title','Useful'])\n",
    "df_result.to_csv(str(filename)+('.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector machines gave good results for a few number of datasets but also it was time consuming because it first forms clusters and then append the remaining data to the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion of algos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear Regression,Logistic Regression and Naive Bayes algorithms were not good approximations because the accuracies were not trustworthy.\n",
    "\n",
    "* Topic Modelling and Support Vector Machines were good algorithms for categorisation but at the same time they were time consuming so either we need to have High graphics and processing machines or we need to jump to deep learning ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering of text by Phonetic and Text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import soundex\n",
    "s =soundex.getInstance()\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import openpyxl\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import difflib\n",
    "from difflib import SequenceMatcher\n",
    "oi=[]\n",
    "gy=[]\n",
    "lo=[]\n",
    "yu=[]\n",
    "filename1=input(\"Enter the name of file in which we have the names of authors in one column1 and in column2\")\n",
    "filename=input(\"Enter the name of file in which you want to view the results\")\n",
    "df=pd.read_csv((str(filename1)+\".csv\"),low_memory=False,encoding=\"LATIN-1\")\n",
    "df1=df[\"author1\"]\n",
    "df3=df[\"author2\"]\n",
    "#phonetics and string similarity\n",
    "def pj():\n",
    "    for each in df1:\n",
    "        for op in df2:\n",
    "            phonetic =s.soundex(str(each))\n",
    "            phonetic1=s.soundex(str(op))\n",
    "            if phonetic==phonetic1:\n",
    "                s1=SequenceMatcher(a=str(each),b=str(op)).ratio()\n",
    "                if s1>0.5:\n",
    "                    oi.append(each)\n",
    "                    gy.append(op)\n",
    "                    lo.append(phonetic)\n",
    "                    yu.append(s1)\n",
    "pj()    \n",
    "df_result=pd.DataFrame(list(zip(oi,gy,lo,yu)),columns=['Author','Author2','Phoneticcode','stringsimilarity'])\n",
    "df_result.to_csv(str(filename)+('.csv'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographical closest location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latitudes and longitudes of a place in ustc masterset which were not closest are compared to latitudes and longitudes of Boscar et.al dataset.Distances are also calculated between those distances in km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import openpyxl\n",
    "from math import sin,cos,sqrt,atan2,radians\n",
    "filename=input(\"Write the name of file in which you want to see the results\")\n",
    "filename1=input(\"Enter the name of file in which we have places of masterset and their locations like latitudes and longitudes\")\n",
    "filename2=input(\"Enter the name of file in which we have places of boscorset and their locations like latitudes and longitudes\")\n",
    "df=pd.read_csv((str(filename1)+\".csv\"),encoding=\"LATIN-1\")\n",
    "df_x=df['Latitude']\n",
    "df_y=df['Longitude']\n",
    "df_z=df['Address']\n",
    "df2=pd.read_csv((str(filename2)+\".csv\"),encoding=\"LATIN-1\")\n",
    "df_x3=df2['latitude']\n",
    "df_y1=df2['longitude']\n",
    "df_z1=df2['city']\n",
    "def findMinimum(l):\n",
    "    a=min (l)\n",
    "    return a\n",
    "i=1\n",
    "re=[]\n",
    "pl=[]\n",
    "zi=[]\n",
    "while(i<190):\n",
    "    x1=float(df_x[i])\n",
    "    y1=float(df_y[i])\n",
    "    j=1\n",
    "    while(j<794):\n",
    "        x2=float(df_x3[j])   \n",
    "        y2=float(df_y1[j])\n",
    "        p1=[x1,y1]\n",
    "        p2=[x2,y2]\n",
    "        R = 6373.0\n",
    "        lat1 = radians(x1)\n",
    "        lon1 = radians(y1)\n",
    "        lat2 = radians(x2)\n",
    "        lon2 = radians(y2)\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = (sin(dlat/2))**2 + cos(lat1) * cos(lat2) * (sin(dlon/2))**2\n",
    "        c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "        distance = R * c\n",
    "        re.append(distance)\n",
    "        pl.append(df_z1[j])\n",
    "        zi.append(df_z[i])\n",
    "        j+=1\n",
    "    i+=1        \n",
    "df_result=pd.DataFrame(list(zip(zi,pl,re)),columns=['Masterplace','Boscorplace','Distance(in km)'])\n",
    "df_result.to_csv(str(filename)+('.csv'))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency of Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most number of words which are repeated in titles are calculated by this code and plotted and can be compared with other titles of books which were printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df=pd.read_csv(\"martinlutherbooks.csv\",encoding='LATIN-1')\n",
    "stop=set(stopwords.words('english'))\n",
    "stop.update([\"a\",\"abord\",\"absolument\",\"afin\",\"ah\",\"ai\",\"aie\",\"ailleurs\",\"ainsi\",\"ait\",\"allaient\",\"allo\",\"allons\",\"allô\",\"alors\",\"anterieur\",\"anterieure\",\"anterieures\",\"apres\",\"après\",\"as\",\"assez\",\"attendu\",\"au\",\"aucun\",\"aucune\",\"aujourd\",\"aujourd'hui\",\"aupres\",\"auquel\",\"aura\",\"auraient\",\"aurait\",\"auront\",\"aussi\",\"autre\",\"autrefois\",\"autrement\",\"autres\",\"autrui\",\"aux\",\"auxquelles\",\"auxquels\",\"avaient\",\"avais\",\"avait\",\"avant\",\"avec\",\"avoir\",\"avons\",\"ayant\",\"b\",\"bah\",\"bas\",\"basee\",\"bat\",\"beau\",\"beaucoup\",\"bien\",\"bigre\",\"boum\",\"bravo\",\"brrr\",\"c\",\"car\",\"ce\",\"ceci\",\"cela\",\"celle\",\"celle-ci\",\"celle-là\",\"celles\",\"celles-ci\",\"celles-là\",\"celui\",\"celui-ci\",\"celui-là\",\"cent\",\"cependant\",\"certain\",\"certaine\",\"certaines\",\"certains\",\"certes\",\"ces\",\"cet\",\"cette\",\"ceux\",\"ceux-ci\",\"ceux-là\",\"chacun\",\"chacune\",\"chaque\",\"cher\",\"chers\",\"chez\",\"chiche\",\"chut\",\"chère\",\"chères\",\"ci\",\"cinq\",\"cinquantaine\",\"cinquante\",\"cinquantième\",\"cinquième\",\"clac\",\"clic\",\"combien\",\"comme\",\"comment\",\"comparable\",\"comparables\",\"compris\",\"concernant\",\"contre\",\"couic\",\"crac\",\"d\",\"da\",\"dans\",\"de\",\"debout\",\"dedans\",\"dehors\",\"deja\",\"delà\",\"depuis\",\"dernier\",\"derniere\",\"derriere\",\"derrière\",\"des\",\"desormais\",\"desquelles\",\"desquels\",\"dessous\",\"dessus\",\"deux\",\"deuxième\",\"deuxièmement\",\"devant\",\"devers\",\"devra\",\"different\",\"differentes\",\"differents\",\"différent\",\"différente\",\"différentes\",\"différents\",\"dire\",\"directe\",\"directement\",\"dit\",\"dite\",\"dits\",\"divers\",\"diverse\",\"diverses\",\"dix\",\"dix-huit\",\"dix-neuf\",\"dix-sept\",\"dixième\",\"doit\",\"doivent\",\"donc\",\"dont\",\"douze\",\"douzième\",\"dring\",\"du\",\"duquel\",\"durant\",\"dès\",\"désormais\",\"e\",\"effet\",\"egale\",\"egalement\",\"egales\",\"eh\",\"elle\",\"elle-même\",\"elles\",\"elles-mêmes\",\"en\",\"encore\",\"enfin\",\"entre\",\"envers\",\"environ\",\"es\",\"est\",\"et\",\"etant\",\"etc\",\"etre\",\"eu\",\"euh\",\"eux\",\"eux-mêmes\",\"exactement\",\"excepté\",\"extenso\",\"exterieur\",\"f\",\"fais\",\"faisaient\",\"faisant\",\"fait\",\"façon\",\"feront\",\"fi\",\"flac\",\"floc\",\"font\",\"g\",\"gens\",\"h\",\"ha\",\"hein\",\"hem\",\"hep\",\"hi\",\"ho\",\"holà\",\"hop\",\"hormis\",\"hors\",\"hou\",\"houp\",\"hue\",\"hui\",\"huit\",\"huitième\",\"hum\",\"hurrah\",\"hé\",\"hélas\",\"i\",\"il\",\"ils\",\"importe\",\"j\",\"je\",\"jusqu\",\"jusque\",\"juste\",\"k\",\"l\",\"la\",\"laisser\",\"laquelle\",\"las\",\"le\",\"lequel\",\"les\",\"lesquelles\",\"lesquels\",\"leur\",\"leurs\",\"longtemps\",\"lors\",\"lorsque\",\"lui\",\"lui-meme\",\"lui-même\",\"là\",\"lès\",\"m\",\"ma\",\"maint\",\"maintenant\",\"mais\",\"malgre\",\"malgré\",\"maximale\",\"me\",\"meme\",\"memes\",\"merci\",\"mes\",\"mien\",\"mienne\",\"miennes\",\"miens\",\"mille\",\"mince\",\"minimale\",\"moi\",\"moi-meme\",\"moi-même\",\"moindres\",\"moins\",\"mon\",\"moyennant\",\"multiple\",\"multiples\",\"même\",\"mêmes\",\"n\",\"na\",\"naturel\",\"naturelle\",\"naturelles\",\"ne\",\"neanmoins\",\"necessaire\",\"necessairement\",\"neuf\",\"neuvième\",\"ni\",\"nombreuses\",\"nombreux\",\"non\",\"nos\",\"notamment\",\"notre\",\"nous\",\"nous-mêmes\",\"nouveau\",\"nul\",\"néanmoins\",\"nôtre\",\"nôtres\",\"o\",\"oh\",\"ohé\",\"ollé\",\"olé\",\"on\",\"ont\",\"onze\",\"onzième\",\"ore\",\"ou\",\"ouf\",\"ouias\",\"oust\",\"ouste\",\"outre\",\"ouvert\",\"ouverte\",\"ouverts\",\"o|\",\"où\",\"p\",\"paf\",\"pan\",\"par\",\"parce\",\"parfois\",\"parle\",\"parlent\",\"parler\",\"parmi\",\"parseme\",\"partant\",\"particulier\",\"particulière\",\"particulièrement\",\"pas\",\"passé\",\"pendant\",\"pense\",\"permet\",\"personne\",\"peu\",\"peut\",\"peuvent\",\"peux\",\"pff\",\"pfft\",\"pfut\",\"pif\",\"pire\",\"plein\",\"plouf\",\"plus\",\"plusieurs\",\"plutôt\",\"possessif\",\"possessifs\",\"possible\",\"possibles\",\"pouah\",\"pour\",\"pourquoi\",\"pourrais\",\"pourrait\",\"pouvait\",\"prealable\",\"precisement\",\"premier\",\"première\",\"premièrement\",\"pres\",\"probable\",\"probante\",\"procedant\",\"proche\",\"près\",\"psitt\",\"pu\",\"puis\",\"puisque\",\"pur\",\"pure\",\"q\",\"qu\",\"quand\",\"quant\",\"quant-à-soi\",\"quanta\",\"quarante\",\"quatorze\",\"quatre\",\"quatre-vingt\",\"quatrième\",\"quatrièmement\",\"que\",\"quel\",\"quelconque\",\"quelle\",\"quelles\",\"quelqu'un\",\"quelque\",\"quelques\",\"quels\",\"qui\",\"quiconque\",\"quinze\",\"quoi\",\"quoique\",\"r\",\"rare\",\"rarement\",\"rares\",\"relative\",\"relativement\",\"remarquable\",\"rend\",\"rendre\",\"restant\",\"reste\",\"restent\",\"restrictif\",\"retour\",\"revoici\",\"revoilà\",\"rien\",\"s\",\"sa\",\"sacrebleu\",\"sait\",\"sans\",\"sapristi\",\"sauf\",\"se\",\"sein\",\"seize\",\"selon\",\"semblable\",\"semblaient\",\"semble\",\"semblent\",\"sent\",\"sept\",\"septième\",\"sera\",\"seraient\",\"serait\",\"seront\",\"ses\",\"seul\",\"seule\",\"seulement\",\"si\",\"sien\",\"sienne\",\"siennes\",\"siens\",\"sinon\",\"six\",\"sixième\",\"soi\",\"soi-même\",\"soit\",\"soixante\",\"son\",\"sont\",\"sous\",\"souvent\",\"specifique\",\"specifiques\",\"speculatif\",\"stop\",\"strictement\",\"subtiles\",\"suffisant\",\"suffisante\",\"suffit\",\"suis\",\"suit\",\"suivant\",\"suivante\",\"suivantes\",\"suivants\",\"suivre\",\"superpose\",\"sur\",\"surtout\",\"t\",\"ta\",\"tac\",\"tant\",\"tardive\",\"te\",\"tel\",\"telle\",\"tellement\",\"telles\",\"tels\",\"tenant\",\"tend\",\"tenir\",\"tente\",\"tes\",\"tic\",\"tien\",\"tienne\",\"tiennes\",\"tiens\",\"toc\",\"toi\",\"toi-même\",\"ton\",\"touchant\",\"toujours\",\"tous\",\"tout\",\"toute\",\"toutefois\",\"toutes\",\"treize\",\"trente\",\"tres\",\"trois\",\"troisième\",\"troisièmement\",\"trop\",\"très\",\"tsoin\",\"tsouin\",\"tu\",\"té\",\"u\",\"un\",\"une\",\"unes\",\"uniformement\",\"unique\",\"uniques\",\"uns\",\"v\",\"va\",\"vais\",\"vas\",\"vers\",\"via\",\"vif\",\"vifs\",\"vingt\",\"vivat\",\"vive\",\"vives\",\"vlan\",\"voici\",\"voilà\",\"vont\",\"vos\",\"votre\",\"vous\",\"vous-mêmes\",\"vu\",\"vé\",\"vôtre\",\"vôtres\",\"w\",\"x\",\"y\",\"z\",\"zut\",\"à\",\"â\",\"ça\",\"ès\",\"étaient\",\"étais\",\"était\",\"étant\",\"été\",\"être\",\"ô\",\"a\",\"actualmente\",\"acuerdo\",\"adelante\",\"ademas\",\"además\",\"adrede\",\"afirmó\",\"agregó\",\"ahi\",\"ahora\",\"ahí\",\"al\",\"algo\",\"alguna\",\"algunas\",\"alguno\",\"algunos\",\"algún\",\"alli\",\"allí\",\"alrededor\",\"ambos\",\"ampleamos\",\"antano\",\"antaño\",\"ante\",\"anterior\",\"antes\",\"apenas\",\"aproximadamente\",\"aquel\",\"aquella\",\"aquellas\",\"aquello\",\"aquellos\",\"aqui\",\"aquél\",\"aquélla\",\"aquéllas\",\"aquéllos\",\"aquí\",\"arriba\",\"arribaabajo\",\"aseguró\",\"asi\",\"así\",\"atras\",\"aun\",\"aunque\",\"ayer\",\"añadió\",\"aún\",\"b\",\"bajo\",\"bastante\",\"bien\",\"breve\",\"buen\",\"buena\",\"buenas\",\"bueno\",\"buenos\",\"c\",\"cada\",\"casi\",\"cerca\",\"cierta\",\"ciertas\",\"cierto\",\"ciertos\",\"cinco\",\"claro\",\"comentó\",\"como\",\"con\",\"conmigo\",\"conocer\",\"conseguimos\",\"conseguir\",\"considera\",\"consideró\",\"consigo\",\"consigue\",\"consiguen\",\"consigues\",\"contigo\",\"contra\",\"cosas\",\"creo\",\"cual\",\"cuales\",\"cualquier\",\"cuando\",\"cuanta\",\"cuantas\",\"cuanto\",\"cuantos\",\"cuatro\",\"cuenta\",\"cuál\",\"cuáles\",\"cuándo\",\"cuánta\",\"cuántas\",\"cuánto\",\"cuántos\",\"cómo\",\"d\",\"da\",\"dado\",\"dan\",\"dar\",\"de\",\"debajo\",\"debe\",\"deben\",\"debido\",\"decir\",\"dejó\",\"del\",\"delante\",\"demasiado\",\"demás\",\"dentro\",\"deprisa\",\"desde\",\"despacio\",\"despues\",\"después\",\"detras\",\"detrás\",\"dia\",\"dias\",\"dice\",\"dicen\",\"dicho\",\"dieron\",\"diferente\",\"diferentes\",\"dijeron\",\"dijo\",\"dio\",\"donde\",\"dos\",\"durante\",\"día\",\"días\",\"dónde\",\"e\",\"ejemplo\",\"el\",\"ella\",\"ellas\",\"ello\",\"ellos\",\"embargo\",\"empleais\",\"emplean\",\"emplear\",\"empleas\",\"empleo\",\"en\",\"encima\",\"encuentra\",\"enfrente\",\"enseguida\",\"entonces\",\"entre\",\"era\",\"eramos\",\"eran\",\"eras\",\"eres\",\"es\",\"esa\",\"esas\",\"ese\",\"eso\",\"esos\",\"esta\",\"estaba\",\"estaban\",\"estado\",\"estados\",\"estais\",\"estamos\",\"estan\",\"estar\",\"estará\",\"estas\",\"este\",\"esto\",\"estos\",\"estoy\",\"estuvo\",\"está\",\"están\",\"ex\",\"excepto\",\"existe\",\"existen\",\"explicó\",\"expresó\",\"f\",\"fin\",\"final\",\"fue\",\"fuera\",\"fueron\",\"fui\",\"fuimos\",\"g\",\"general\",\"gran\",\"grandes\",\"gueno\",\"h\",\"ha\",\"haber\",\"habia\",\"habla\",\"hablan\",\"habrá\",\"había\",\"habían\",\"hace\",\"haceis\",\"hacemos\",\"hacen\",\"hacer\",\"hacerlo\",\"haces\",\"hacia\",\"haciendo\",\"hago\",\"han\",\"hasta\",\"hay\",\"haya\",\"he\",\"hecho\",\"hemos\",\"hicieron\",\"hizo\",\"horas\",\"hoy\",\"hubo\",\"i\",\"igual\",\"incluso\",\"indicó\",\"informo\",\"informó\",\"intenta\",\"intentais\",\"intentamos\",\"intentan\",\"intentar\",\"intentas\",\"intento\",\"ir\",\"j\",\"junto\",\"k\",\"l\",\"la\",\"lado\",\"largo\",\"las\",\"le\",\"lejos\",\"les\",\"llegó\",\"lleva\",\"llevar\",\"lo\",\"los\",\"luego\",\"lugar\",\"m\",\"mal\",\"manera\",\"manifestó\",\"mas\",\"mayor\",\"me\",\"mediante\",\"medio\",\"mejor\",\"mencionó\",\"menos\",\"menudo\",\"mi\",\"mia\",\"mias\",\"mientras\",\"mio\",\"mios\",\"mis\",\"misma\",\"mismas\",\"mismo\",\"mismos\",\"modo\",\"momento\",\"mucha\",\"muchas\",\"mucho\",\"muchos\",\"muy\",\"más\",\"mí\",\"mía\",\"mías\",\"mío\",\"míos\",\"n\",\"nada\",\"nadie\",\"ni\",\"ninguna\",\"ningunas\",\"ninguno\",\"ningunos\",\"ningún\",\"no\",\"nos\",\"nosotras\",\"nosotros\",\"nuestra\",\"nuestras\",\"nuestro\",\"nuestros\",\"nueva\",\"nuevas\",\"nuevo\",\"nuevos\",\"nunca\",\"o\",\"ocho\",\"os\",\"otra\",\"otras\",\"otro\",\"otros\",\"p\",\"pais\",\"para\",\"parece\",\"parte\",\"partir\",\"pasada\",\"pasado\",\"paìs\",\"peor\",\"pero\",\"pesar\",\"poca\",\"pocas\",\"poco\",\"pocos\",\"podeis\",\"podemos\",\"poder\",\"podria\",\"podriais\",\"podriamos\",\"podrian\",\"podrias\",\"podrá\",\"podrán\",\"podría\",\"podrían\",\"poner\",\"por\",\"porque\",\"posible\",\"primer\",\"primera\",\"primero\",\"primeros\",\"principalmente\",\"pronto\",\"propia\",\"propias\",\"propio\",\"propios\",\"proximo\",\"próximo\",\"próximos\",\"pudo\",\"pueda\",\"puede\",\"pueden\",\"puedo\",\"pues\",\"q\",\"qeu\",\"que\",\"quedó\",\"queremos\",\"quien\",\"quienes\",\"quiere\",\"quiza\",\"quizas\",\"quizá\",\"quizás\",\"quién\",\"quiénes\",\"qué\",\"r\",\"raras\",\"realizado\",\"realizar\",\"realizó\",\"repente\",\"respecto\",\"s\",\"sabe\",\"sabeis\",\"sabemos\",\"saben\",\"saber\",\"sabes\",\"salvo\",\"se\",\"sea\",\"sean\",\"segun\",\"segunda\",\"segundo\",\"según\",\"seis\",\"ser\",\"sera\",\"será\",\"serán\",\"sería\",\"señaló\",\"si\",\"sido\",\"siempre\",\"siendo\",\"siete\",\"sigue\",\"siguiente\",\"sin\",\"sino\",\"sobre\",\"sois\",\"sola\",\"solamente\",\"solas\",\"solo\",\"solos\",\"somos\",\"son\",\"soy\",\"soyos\",\"su\",\"supuesto\",\"sus\",\"suya\",\"suyas\",\"suyo\",\"sé\",\"sí\",\"sólo\",\"t\",\"tal\",\"tambien\",\"también\",\"tampoco\",\"tan\",\"tanto\",\"tarde\",\"te\",\"temprano\",\"tendrá\",\"tendrán\",\"teneis\",\"tenemos\",\"tener\",\"tenga\",\"tengo\",\"tenido\",\"tenía\",\"tercera\",\"ti\",\"tiempo\",\"tiene\",\"tienen\",\"toda\",\"todas\",\"todavia\",\"todavía\",\"todo\",\"todos\",\"total\",\"trabaja\",\"trabajais\",\"trabajamos\",\"trabajan\",\"trabajar\",\"trabajas\",\"trabajo\",\"tras\",\"trata\",\"través\",\"tres\",\"tu\",\"tus\",\"tuvo\",\"tuya\",\"tuyas\",\"tuyo\",\"tuyos\",\"tú\",\"u\",\"ultimo\",\"un\",\"una\",\"unas\",\"uno\",\"unos\",\"usa\",\"usais\",\"usamos\",\"usan\",\"usar\",\"usas\",\"uso\",\"usted\",\"ustedes\",\"v\",\"va\",\"vais\",\"valor\",\"vamos\",\"van\",\"varias\",\"varios\",\"vaya\",\"veces\",\"ver\",\"verdad\",\"verdadera\",\"verdadero\",\"vez\",\"vosotras\",\"vosotros\",\"voy\",\"vuestra\",\"vuestras\",\"vuestro\",\"vuestros\",\"w\",\"x\",\"y\",\"ya\",\"yo\",\"z\",\"él\",\"ésa\",\"ésas\",\"ése\",\"ésos\",\"ésta\",\"éstas\",\"éste\",\"éstos\",\"última\",\"últimas\",\"último\",\"últimos\",\"Ernst\",\"Ordnung\",\"Schluss\",\"a\",\"ab\",\"aber\",\"ach\",\"acht\",\"achte\",\"achten\",\"achter\",\"achtes\",\"ag\",\"alle\",\"allein\",\"allem\",\"allen\",\"aller\",\"allerdings\",\"alles\",\"allgemeinen\",\"als\",\"also\",\"am\",\"an\",\"andere\",\"anderen\",\"andern\",\"anders\",\"au\",\"auch\",\"auf\",\"aus\",\"ausser\",\"ausserdem\",\"außer\",\"außerdem\",\"b\",\"bald\",\"bei\",\"beide\",\"beiden\",\"beim\",\"beispiel\",\"bekannt\",\"bereits\",\"besonders\",\"besser\",\"besten\",\"bin\",\"bis\",\"bisher\",\"bist\",\"c\",\"d\",\"d.h\",\"da\",\"dabei\",\"dadurch\",\"dafür\",\"dagegen\",\"daher\",\"dahin\",\"dahinter\",\"damals\",\"damit\",\"danach\",\"daneben\",\"dank\",\"dann\",\"daran\",\"darauf\",\"daraus\",\"darf\",\"darfst\",\"darin\",\"darum\",\"darunter\",\"darüber\",\"das\",\"dasein\",\"daselbst\",\"dass\",\"dasselbe\",\"davon\",\"davor\",\"dazu\",\"dazwischen\",\"daß\",\"dein\",\"deine\",\"deinem\",\"deiner\",\"dem\",\"dementsprechend\",\"demgegenüber\",\"demgemäss\",\"demgemäß\",\"demselben\",\"demzufolge\",\"den\",\"denen\",\"denn\",\"denselben\",\"der\",\"deren\",\"derjenige\",\"derjenigen\",\"dermassen\",\"dermaßen\",\"derselbe\",\"derselben\",\"des\",\"deshalb\",\"desselben\",\"dessen\",\"deswegen\",\"dich\",\"die\",\"diejenige\",\"diejenigen\",\"dies\",\"diese\",\"dieselbe\",\"dieselben\",\"diesem\",\"diesen\",\"dieser\",\"dieses\",\"dir\",\"doch\",\"dort\",\"drei\",\"drin\",\"dritte\",\"dritten\",\"dritter\",\"drittes\",\"du\",\"durch\",\"durchaus\",\"durfte\",\"durften\",\"dürfen\",\"dürft\",\"e\",\"eben\",\"ebenso\",\"ehrlich\",\"ei\",\"ei,\",\"eigen\",\"eigene\",\"eigenen\",\"eigener\",\"eigenes\",\"ein\",\"einander\",\"eine\",\"einem\",\"einen\",\"einer\",\"eines\",\"einige\",\"einigen\",\"einiger\",\"einiges\",\"einmal\",\"eins\",\"elf\",\"en\",\"ende\",\"endlich\",\"entweder\",\"er\",\"erst\",\"erste\",\"ersten\",\"erster\",\"erstes\",\"es\",\"etwa\",\"etwas\",\"euch\",\"euer\",\"eure\",\"f\",\"folgende\",\"früher\",\"fünf\",\"fünfte\",\"fünften\",\"fünfter\",\"fünftes\",\"für\",\"g\",\"gab\",\"ganz\",\"ganze\",\"ganzen\",\"ganzer\",\"ganzes\",\"gar\",\"gedurft\",\"gegen\",\"gegenüber\",\"gehabt\",\"gehen\",\"geht\",\"gekannt\",\"gekonnt\",\"gemacht\",\"gemocht\",\"gemusst\",\"genug\",\"gerade\",\"gern\",\"gesagt\",\"geschweige\",\"gewesen\",\"gewollt\",\"geworden\",\"gibt\",\"ging\",\"gleich\",\"gott\",\"gross\",\"grosse\",\"grossen\",\"grosser\",\"grosses\",\"groß\",\"große\",\"großen\",\"großer\",\"großes\",\"gut\",\"gute\",\"guter\",\"gutes\",\"h\",\"habe\",\"haben\",\"habt\",\"hast\",\"hat\",\"hatte\",\"hatten\",\"hattest\",\"hattet\",\"heisst\",\"her\",\"heute\",\"hier\",\"hin\",\"hinter\",\"hoch\",\"hätte\",\"hätten\",\"i\",\"ich\",\"ihm\",\"ihn\",\"ihnen\",\"ihr\",\"ihre\",\"ihrem\",\"ihren\",\"ihrer\",\"ihres\",\"im\",\"immer\",\"in\",\"indem\",\"infolgedessen\",\"ins\",\"irgend\",\"ist\",\"j\",\"ja\",\"jahr\",\"jahre\",\"jahren\",\"je\",\"jede\",\"jedem\",\"jeden\",\"jeder\",\"jedermann\",\"jedermanns\",\"jedes\",\"jedoch\",\"jemand\",\"jemandem\",\"jemanden\",\"jene\",\"jenem\",\"jenen\",\"jener\",\"jenes\",\"jetzt\",\"k\",\"kam\",\"kann\",\"kannst\",\"kaum\",\"kein\",\"keine\",\"keinem\",\"keinen\",\"keiner\",\"kleine\",\"kleinen\",\"kleiner\",\"kleines\",\"kommen\",\"kommt\",\"konnte\",\"konnten\",\"kurz\",\"können\",\"könnt\",\"könnte\",\"l\",\"lang\",\"lange\",\"leicht\",\"leide\",\"lieber\",\"los\",\"m\",\"machen\",\"macht\",\"machte\",\"mag\",\"magst\",\"mahn\",\"mal\",\"man\",\"manche\",\"manchem\",\"manchen\",\"mancher\",\"manches\",\"mann\",\"mehr\",\"mein\",\"meine\",\"meinem\",\"meinen\",\"meiner\",\"meines\",\"mensch\",\"menschen\",\"mich\",\"mir\",\"mit\",\"mittel\",\"mochte\",\"mochten\",\"morgen\",\"muss\",\"musst\",\"musste\",\"mussten\",\"muß\",\"mußt\",\"möchte\",\"mögen\",\"möglich\",\"mögt\",\"müssen\",\"müsst\",\"müßt\",\"n\",\"na\",\"nach\",\"nachdem\",\"nahm\",\"natürlich\",\"neben\",\"nein\",\"neue\",\"neuen\",\"neun\",\"neunte\",\"neunten\",\"neunter\",\"neuntes\",\"nicht\",\"nichts\",\"nie\",\"niemand\",\"niemandem\",\"niemanden\",\"noch\",\"nun\",\"nur\",\"o\",\"ob\",\"oben\",\"oder\",\"offen\",\"oft\",\"ohne\",\"p\",\"q\",\"r\",\"recht\",\"rechte\",\"rechten\",\"rechter\",\"rechtes\",\"richtig\",\"rund\",\"s\",\"sa\",\"sache\",\"sagt\",\"sagte\",\"sah\",\"satt\",\"schlecht\",\"schon\",\"sechs\",\"sechste\",\"sechsten\",\"sechster\",\"sechstes\",\"sehr\",\"sei\",\"seid\",\"seien\",\"sein\",\"seine\",\"seinem\",\"seinen\",\"seiner\",\"seines\",\"seit\",\"seitdem\",\"selbst\",\"sich\",\"sie\",\"sieben\",\"siebente\",\"siebenten\",\"siebenter\",\"siebentes\",\"sind\",\"so\",\"solang\",\"solche\",\"solchem\",\"solchen\",\"solcher\",\"solches\",\"soll\",\"sollen\",\"sollst\",\"sollt\",\"sollte\",\"sollten\",\"sondern\",\"sonst\",\"soweit\",\"sowie\",\"später\",\"startseite\",\"statt\",\"steht\",\"suche\",\"t\",\"tag\",\"tage\",\"tagen\",\"tat\",\"teil\",\"tel\",\"tritt\",\"trotzdem\",\"tun\",\"u\",\"uhr\",\"um\",\"und\",\"und?\",\"uns\",\"unser\",\"unsere\",\"unserer\",\"unter\",\"v\",\"vergangenen\",\"viel\",\"viele\",\"vielem\",\"vielen\",\"vielleicht\",\"vier\",\"vierte\",\"vierten\",\"vierter\",\"viertes\",\"vom\",\"von\",\"vor\",\"w\",\"wahr?\",\"wann\",\"war\",\"waren\",\"wart\",\"warum\",\"was\",\"wegen\",\"weil\",\"weit\",\"weiter\",\"weitere\",\"weiteren\",\"weiteres\",\"welche\",\"welchem\",\"welchen\",\"welcher\",\"welches\",\"wem\",\"wen\",\"wenig\",\"wenige\",\"weniger\",\"weniges\",\"wenigstens\",\"wenn\",\"wer\",\"werde\",\"werden\",\"werdet\",\"weshalb\",\"wessen\",\"wie\",\"wieder\",\"wieso\",\"will\",\"willst\",\"wir\",\"wird\",\"wirklich\",\"wirst\",\"wissen\",\"wo\",\"wohl\",\"wollen\",\"wollt\",\"wollte\",\"wollten\",\"worden\",\"wurde\",\"wurden\",\"während\",\"währenddem\",\"währenddessen\",\"wäre\",\"würde\",\"würden\",\"x\",\"y\",\"z\",\"z.b\",\"zehn\",\"zehnte\",\"zehnten\",\"zehnter\",\"zehntes\",\"zeit\",\"zu\",\"zuerst\",\"zugleich\",\"zum\",\"zunächst\",\"zur\",\"zurück\",\"zusammen\",\"zwanzig\",\"zwar\",\"zwei\",\"zweite\",\"zweiten\",\"zweiter\",\"zweites\",\"zwischen\",\"zwölf\",\"über\",\"überhaupt\",\"übrigens\"])\n",
    "df_x=df['Title']\n",
    "aw=df_x.str.cat(sep=' ')\n",
    "list_of_words=[i.lower() for i in wordpunct_tokenize(aw) if i.lower() not in stop and i.isalpha()]\n",
    "wordfreqdist = nltk.FreqDist(list_of_words)\n",
    "mostcommon = wordfreqdist.most_common(30)\n",
    "print(mostcommon)\n",
    "plt.barh(range(len(mostcommon)),[val[1] for val in mostcommon], align='center')\n",
    "plt.yticks(range(len(mostcommon)), [val[0] for val in mostcommon])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Matching Categorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just look at these two texts-\n",
    "\n",
    "* Categorised-Naufragio da nao n. senhora de Belem, feito na terra do Natal no cabo de Boa esperaní?a, & varios sucessos que teve o capití£o Joseph d...Lisboa,por Lourení?o Craesbeeck?, 1636?.\n",
    "\n",
    "* Uncategorised-Naufragio da nao nosso senhora de Bethlem. Feito na terra do Natal no cabo de Boa esperaní?a successos que teve o capitaíµ Joseph de Ca...Lisboa, por Lourení?o Craesbeeck, 1636\n",
    "\n",
    "\n",
    "One of them is from the texts which have been categorized and one of them is from those texts which are uncategorized. So,as these two titles resemble each other then it means that we can use string similarity for comparing and grouping them as same categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import difflib\n",
    "from difflib import SequenceMatcher\n",
    "##s1=SequenceMatcher(a=str(each),b=str(er)).ratio()\n",
    "filename=input(\"Please enter the name of file in which we have categories as named\")\n",
    "filename1=input(\"Please enter the name of file in which we dont have the categories named\")\n",
    "filename3=input(\"Please enter the name of file in which you want to see the results\")\n",
    "df=pd.read_csv((str(filename)+\".csv\"),encoding='LATIN-1',low_memory=False)\n",
    "df1=pd.read_csv((str(filename1)+\".csv\"),encoding='LATIN-1',low_memory=False)\n",
    "df_x=df1['Author']\n",
    "df_y=df['Concat']\n",
    "df_z=df1['Place']\n",
    "df_v=df1['Year']\n",
    "df_u=df['category']\n",
    "df_e=df1['Concat']\n",
    "df_q=df1['Title']\n",
    "df_r=df['title']\n",
    "df_wq=df['author']\n",
    "j=0\n",
    "oi=[]\n",
    "po=[]\n",
    "ol=[]\n",
    "we=[]\n",
    "wq=[]\n",
    "yt=[]\n",
    "lp=[]\n",
    "while(j<127354):\n",
    "    i=0\n",
    "    while(i<331552):\n",
    "        if df_x[j]==df_wq[i]:\n",
    "            s1=SequenceMatcher(a=str(df_y[i]),b=str(df_e[j])).ratio()\n",
    "            if s1>0.75:\n",
    "                print(df_x[j])\n",
    "                print(df_e[j])\n",
    "                print(df_z[j])\n",
    "                print(df_v[j])\n",
    "                print(df_u[i])\n",
    "                print(df_y[i])\n",
    "                print(s1)\n",
    "                oi.append(df_x[j])\n",
    "                po.append(df_q[j])\n",
    "                ol.append(df_z[j])\n",
    "                we.append(df_v[j])\n",
    "                wq.append(df_u[i])\n",
    "                yt.append(df_r[i])\n",
    "                lp.append(s1)\n",
    "        i+=1\n",
    "    j+=1\n",
    "df_result=pd.DataFrame(list(zip(oi,po,ol,we,wq,yt,lp)),columns=['Author','Uncattitle','Place','Year','Category','Cattitle','Stringsimilarity'])\n",
    "df_result.to_csv(str(filename3)+('.csv'))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radicalness of author,place and language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that we shall prepare our desired corpuses sorting by places,authors,languages and categories yearwise and try to see that how the titles of books keeps on changing year to year and what is the trend of radicalness or intersection of most common words(In the code = 10 but can be changed willingly).But remember that more the number of words more will be the time you need to wait for the results.You can then plot a graph and see the results for yourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of place or write allplaces or write nanWittenberg\n",
      "Enter the name of author or write allauthors or write nanallauthors\n",
      "Enter the name of language or write alllanguages or write nanalllanguages\n",
      "Choose among multicategories and singlecategory,for allcategories, nan and only unique categories choose write singlecategory othervise for intersection choose multicategory\n",
      "For category would you prefer multicategory or singlecategorysinglecategory\n",
      "Enter the name of categoryallcategories\n",
      "Please enter the name of a new file in which you want to save the dataWittenberg28\n",
      "The code has started. Please wait for sometime\n",
      "Stopwords are updated\n",
      "The places have been sorted\n",
      "The authors have been sorted\n",
      "The Languages have been sorted\n",
      "The categories have been sorted\n",
      "The code has successfully ended . You may now open the desired file and see your expected results\n"
     ]
    }
   ],
   "source": [
    "from pandas import ExcelWriter\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "place=input(\"Enter the name of place or write allplaces or write nan\")\n",
    "author=input(\"Enter the name of author or write allauthors or write nan\")\n",
    "language=input(\"Enter the name of language or write alllanguages or write nan\")\n",
    "print(\"Choose among multicategories and singlecategory,for allcategories, nan and only unique categories choose write singlecategory othervise for intersection choose multicategory\")\n",
    "choice1=input(\"For category would you prefer multicategory or singlecategory\")\n",
    "if choice1==\"multicategory\":\n",
    "    category=input(\"Enter the name of categories separated by commas\")\n",
    "if choice1==\"singlecategory\":\n",
    "    category=input(\"Enter the name of category\")\n",
    "if choice1=='singlecategory':\n",
    "    plm=category\n",
    "input_list=category.split(',')    \n",
    "filename=input(\"Please enter the name of a new file in which you want to save the data\")\n",
    "print(\"The code has started. Please wait for sometime\")\n",
    "tgb='allplaces'\n",
    "yhn='allauthors'\n",
    "ujm='alllanguages'\n",
    "rty='allcategories'\n",
    "wsx='nan'\n",
    "wer=language\n",
    "if category==rty:\n",
    "    plm=category\n",
    "elif category==wsx:\n",
    "    plm=category\n",
    "if language==ujm:\n",
    "    wer=language\n",
    "elif language==wsx:\n",
    "    wer=language\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"ustcmasterfullyearschanged.csv\",low_memory=False,encoding='LATIN-1')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,wordpunct_tokenize,RegexpTokenizer\n",
    "tokenizer1=RegexpTokenizer(r'\\w+')\n",
    "df1=pd.read_csv(\"stopwords.csv\",low_memory=False,encoding='LATIN-1')\n",
    "df1_a=df1['yearstop']\n",
    "df1_b=df1['placestop']\n",
    "df1_c=df1['author']\n",
    "stop=set(stopwords.words('english'))\n",
    "df_a=df['author']\n",
    "df_b=df['title']\n",
    "df_c=df['place']\n",
    "df_d=df['year']\n",
    "df_e=df['lang2']\n",
    "df_f=df['cat2']\n",
    "df_g=df['multilang']\n",
    "df_h=df['placebooks']\n",
    "stop.update([\"a\",\"ainda\",\"alem\",\"ambas\",\"ambos\",\"antes\",\"ao\",\"aonde\",\"aos\",\"apos\",\"aquele\",\"aqueles\",\"as\",\"assim\",\"com\",\"como\",\"contra\",\"contudo\",\"cuja\",\"cujas\",\"cujo\",\"cujos\",\"da\",\"das\",\"de\",\"dela\",\"dele\",\"deles\",\"demais\",\"depois\",\"desde\",\"desta\",\"deste\",\"dispoe\",\"dispoem\",\"diversa\",\"diversas\",\"diversos\",\"do\",\"dos\",\"durante\",\"e\",\"ela\",\"elas\",\"ele\",\"eles\",\"em\",\"entao\",\"entre\",\"essa\",\"essas\",\"esse\",\"esses\",\"esta\",\"estas\",\"este\",\"estes\",\"ha\",\"isso\",\"isto\",\"logo\",\"mais\",\"mas\",\"mediante\",\"menos\",\"mesma\",\"mesmas\",\"mesmo\",\"mesmos\",\"na\",\"nao\",\"nas\",\"nem\",\"nesse\",\"neste\",\"nos\",\"o\",\"os\",\"ou\",\"outra\",\"outras\",\"outro\",\"outros\",\"pelas\",\"pelo\",\"pelos\",\"perante\",\"pois\",\"por\",\"porque\",\"portanto\",\"propios\",\"proprio\",\"quais\",\"qual\",\"qualquer\",\"quando\",\"quanto\",\"que\",\"quem\",\"quer\",\"se\",\"seja\",\"sem\",\"sendo\",\"seu\",\"seus\",\"sob\",\"sobre\",\"sua\",\"suas\",\"tal\",\"tambem\",\"teu\",\"teus\",\"toda\",\"todas\",\"todo\",\"todos\",\"tua\",\"tuas\",\"tudo\",\"um\",\"uma\",\"umas\",\"uns\",\"a\",\"abord\",\"absolument\",\"afin\",\"ah\",\"ai\",\"aie\",\"ailleurs\",\"ainsi\",\"ait\",\"allaient\",\"allo\",\"allons\",\"allô\",\"alors\",\"anterieur\",\"anterieure\",\"anterieures\",\"apres\",\"après\",\"as\",\"assez\",\"attendu\",\"au\",\"aucun\",\"aucune\",\"aujourd\",\"aujourd'hui\",\"aupres\",\"auquel\",\"aura\",\"auraient\",\"aurait\",\"auront\",\"aussi\",\"autre\",\"autrefois\",\"autrement\",\"autres\",\"autrui\",\"aux\",\"auxquelles\",\"auxquels\",\"avaient\",\"avais\",\"avait\",\"avant\",\"avec\",\"avoir\",\"avons\",\"ayant\",\"b\",\"bah\",\"bas\",\"basee\",\"bat\",\"beau\",\"beaucoup\",\"bien\",\"bigre\",\"boum\",\"bravo\",\"brrr\",\"c\",\"car\",\"ce\",\"ceci\",\"cela\",\"celle\",\"celle-ci\",\"celle-là\",\"celles\",\"celles-ci\",\"celles-là\",\"celui\",\"celui-ci\",\"celui-là\",\"cent\",\"cependant\",\"certain\",\"certaine\",\"certaines\",\"certains\",\"certes\",\"ces\",\"cet\",\"cette\",\"ceux\",\"ceux-ci\",\"ceux-là\",\"chacun\",\"chacune\",\"chaque\",\"cher\",\"chers\",\"chez\",\"chiche\",\"chut\",\"chère\",\"chères\",\"ci\",\"cinq\",\"cinquantaine\",\"cinquante\",\"cinquantième\",\"cinquième\",\"clac\",\"clic\",\"combien\",\"comme\",\"comment\",\"comparable\",\"comparables\",\"compris\",\"concernant\",\"contre\",\"couic\",\"crac\",\"d\",\"da\",\"dans\",\"de\",\"debout\",\"dedans\",\"dehors\",\"deja\",\"delà\",\"depuis\",\"dernier\",\"derniere\",\"derriere\",\"derrière\",\"des\",\"desormais\",\"desquelles\",\"desquels\",\"dessous\",\"dessus\",\"deux\",\"deuxième\",\"deuxièmement\",\"devant\",\"devers\",\"devra\",\"different\",\"differentes\",\"differents\",\"différent\",\"différente\",\"différentes\",\"différents\",\"dire\",\"directe\",\"directement\",\"dit\",\"dite\",\"dits\",\"divers\",\"diverse\",\"diverses\",\"dix\",\"dix-huit\",\"dix-neuf\",\"dix-sept\",\"dixième\",\"doit\",\"doivent\",\"donc\",\"dont\",\"douze\",\"douzième\",\"dring\",\"du\",\"duquel\",\"durant\",\"dès\",\"désormais\",\"e\",\"effet\",\"egale\",\"egalement\",\"egales\",\"eh\",\"elle\",\"elle-même\",\"elles\",\"elles-mêmes\",\"en\",\"encore\",\"enfin\",\"entre\",\"envers\",\"environ\",\"es\",\"est\",\"et\",\"etant\",\"etc\",\"etre\",\"eu\",\"euh\",\"eux\",\"eux-mêmes\",\"exactement\",\"excepté\",\"extenso\",\"exterieur\",\"f\",\"fais\",\"faisaient\",\"faisant\",\"fait\",\"façon\",\"feront\",\"fi\",\"flac\",\"floc\",\"font\",\"g\",\"gens\",\"h\",\"ha\",\"hein\",\"hem\",\"hep\",\"hi\",\"ho\",\"holà\",\"hop\",\"hormis\",\"hors\",\"hou\",\"houp\",\"hue\",\"hui\",\"huit\",\"huitième\",\"hum\",\"hurrah\",\"hé\",\"hélas\",\"i\",\"il\",\"ils\",\"importe\",\"j\",\"je\",\"jusqu\",\"jusque\",\"juste\",\"k\",\"l\",\"la\",\"laisser\",\"laquelle\",\"las\",\"le\",\"lequel\",\"les\",\"lesquelles\",\"lesquels\",\"leur\",\"leurs\",\"longtemps\",\"lors\",\"lorsque\",\"lui\",\"lui-meme\",\"lui-même\",\"là\",\"lès\",\"m\",\"ma\",\"maint\",\"maintenant\",\"mais\",\"malgre\",\"malgré\",\"maximale\",\"me\",\"meme\",\"memes\",\"merci\",\"mes\",\"mien\",\"mienne\",\"miennes\",\"miens\",\"mille\",\"mince\",\"minimale\",\"moi\",\"moi-meme\",\"moi-même\",\"moindres\",\"moins\",\"mon\",\"moyennant\",\"multiple\",\"multiples\",\"même\",\"mêmes\",\"n\",\"na\",\"naturel\",\"naturelle\",\"naturelles\",\"ne\",\"neanmoins\",\"necessaire\",\"necessairement\",\"neuf\",\"neuvième\",\"ni\",\"nombreuses\",\"nombreux\",\"non\",\"nos\",\"notamment\",\"notre\",\"nous\",\"nous-mêmes\",\"nouveau\",\"nul\",\"néanmoins\",\"nôtre\",\"nôtres\",\"o\",\"oh\",\"ohé\",\"ollé\",\"olé\",\"on\",\"ont\",\"onze\",\"onzième\",\"ore\",\"ou\",\"ouf\",\"ouias\",\"oust\",\"ouste\",\"outre\",\"ouvert\",\"ouverte\",\"ouverts\",\"o|\",\"où\",\"p\",\"paf\",\"pan\",\"par\",\"parce\",\"parfois\",\"parle\",\"parlent\",\"parler\",\"parmi\",\"parseme\",\"partant\",\"particulier\",\"particulière\",\"particulièrement\",\"pas\",\"passé\",\"pendant\",\"pense\",\"permet\",\"personne\",\"peu\",\"peut\",\"peuvent\",\"peux\",\"pff\",\"pfft\",\"pfut\",\"pif\",\"pire\",\"plein\",\"plouf\",\"plus\",\"plusieurs\",\"plutôt\",\"possessif\",\"possessifs\",\"possible\",\"possibles\",\"pouah\",\"pour\",\"pourquoi\",\"pourrais\",\"pourrait\",\"pouvait\",\"prealable\",\"precisement\",\"premier\",\"première\",\"premièrement\",\"pres\",\"probable\",\"probante\",\"procedant\",\"proche\",\"près\",\"psitt\",\"pu\",\"puis\",\"puisque\",\"pur\",\"pure\",\"q\",\"qu\",\"quand\",\"quant\",\"quant-à-soi\",\"quanta\",\"quarante\",\"quatorze\",\"quatre\",\"quatre-vingt\",\"quatrième\",\"quatrièmement\",\"que\",\"quel\",\"quelconque\",\"quelle\",\"quelles\",\"quelqu'un\",\"quelque\",\"quelques\",\"quels\",\"qui\",\"quiconque\",\"quinze\",\"quoi\",\"quoique\",\"r\",\"rare\",\"rarement\",\"rares\",\"relative\",\"relativement\",\"remarquable\",\"rend\",\"rendre\",\"restant\",\"reste\",\"restent\",\"restrictif\",\"retour\",\"revoici\",\"revoilà\",\"rien\",\"s\",\"sa\",\"sacrebleu\",\"sait\",\"sans\",\"sapristi\",\"sauf\",\"se\",\"sein\",\"seize\",\"selon\",\"semblable\",\"semblaient\",\"semble\",\"semblent\",\"sent\",\"sept\",\"septième\",\"sera\",\"seraient\",\"serait\",\"seront\",\"ses\",\"seul\",\"seule\",\"seulement\",\"si\",\"sien\",\"sienne\",\"siennes\",\"siens\",\"sinon\",\"six\",\"sixième\",\"soi\",\"soi-même\",\"soit\",\"soixante\",\"son\",\"sont\",\"sous\",\"souvent\",\"specifique\",\"specifiques\",\"speculatif\",\"stop\",\"strictement\",\"subtiles\",\"suffisant\",\"suffisante\",\"suffit\",\"suis\",\"suit\",\"suivant\",\"suivante\",\"suivantes\",\"suivants\",\"suivre\",\"superpose\",\"sur\",\"surtout\",\"t\",\"ta\",\"tac\",\"tant\",\"tardive\",\"te\",\"tel\",\"telle\",\"tellement\",\"telles\",\"tels\",\"tenant\",\"tend\",\"tenir\",\"tente\",\"tes\",\"tic\",\"tien\",\"tienne\",\"tiennes\",\"tiens\",\"toc\",\"toi\",\"toi-même\",\"ton\",\"touchant\",\"toujours\",\"tous\",\"tout\",\"toute\",\"toutefois\",\"toutes\",\"treize\",\"trente\",\"tres\",\"trois\",\"troisième\",\"troisièmement\",\"trop\",\"très\",\"tsoin\",\"tsouin\",\"tu\",\"té\",\"u\",\"un\",\"une\",\"unes\",\"uniformement\",\"unique\",\"uniques\",\"uns\",\"v\",\"va\",\"vais\",\"vas\",\"vers\",\"via\",\"vif\",\"vifs\",\"vingt\",\"vivat\",\"vive\",\"vives\",\"vlan\",\"voici\",\"voilà\",\"vont\",\"vos\",\"votre\",\"vous\",\"vous-mêmes\",\"vu\",\"vé\",\"vôtre\",\"vôtres\",\"w\",\"x\",\"y\",\"z\",\"zut\",\"à\",\"â\",\"ça\",\"ès\",\"étaient\",\"étais\",\"était\",\"étant\",\"été\",\"être\",\"ô\",\"a\",\"actualmente\",\"acuerdo\",\"adelante\",\"ademas\",\"además\",\"adrede\",\"afirmó\",\"agregó\",\"ahi\",\"ahora\",\"ahí\",\"al\",\"algo\",\"alguna\",\"algunas\",\"alguno\",\"algunos\",\"algún\",\"alli\",\"allí\",\"alrededor\",\"ambos\",\"ampleamos\",\"antano\",\"antaño\",\"ante\",\"anterior\",\"antes\",\"apenas\",\"aproximadamente\",\"aquel\",\"aquella\",\"aquellas\",\"aquello\",\"aquellos\",\"aqui\",\"aquél\",\"aquélla\",\"aquéllas\",\"aquéllos\",\"aquí\",\"arriba\",\"arribaabajo\",\"aseguró\",\"asi\",\"así\",\"atras\",\"aun\",\"aunque\",\"ayer\",\"añadió\",\"aún\",\"b\",\"bajo\",\"bastante\",\"bien\",\"breve\",\"buen\",\"buena\",\"buenas\",\"bueno\",\"buenos\",\"c\",\"cada\",\"casi\",\"cerca\",\"cierta\",\"ciertas\",\"cierto\",\"ciertos\",\"cinco\",\"claro\",\"comentó\",\"como\",\"con\",\"conmigo\",\"conocer\",\"conseguimos\",\"conseguir\",\"considera\",\"consideró\",\"consigo\",\"consigue\",\"consiguen\",\"consigues\",\"contigo\",\"contra\",\"cosas\",\"creo\",\"cual\",\"cuales\",\"cualquier\",\"cuando\",\"cuanta\",\"cuantas\",\"cuanto\",\"cuantos\",\"cuatro\",\"cuenta\",\"cuál\",\"cuáles\",\"cuándo\",\"cuánta\",\"cuántas\",\"cuánto\",\"cuántos\",\"cómo\",\"d\",\"da\",\"dado\",\"dan\",\"dar\",\"de\",\"debajo\",\"debe\",\"deben\",\"debido\",\"decir\",\"dejó\",\"del\",\"delante\",\"demasiado\",\"demás\",\"dentro\",\"deprisa\",\"desde\",\"despacio\",\"despues\",\"después\",\"detras\",\"detrás\",\"dia\",\"dias\",\"dice\",\"dicen\",\"dicho\",\"dieron\",\"diferente\",\"diferentes\",\"dijeron\",\"dijo\",\"dio\",\"donde\",\"dos\",\"durante\",\"día\",\"días\",\"dónde\",\"e\",\"ejemplo\",\"el\",\"ella\",\"ellas\",\"ello\",\"ellos\",\"embargo\",\"empleais\",\"emplean\",\"emplear\",\"empleas\",\"empleo\",\"en\",\"encima\",\"encuentra\",\"enfrente\",\"enseguida\",\"entonces\",\"entre\",\"era\",\"eramos\",\"eran\",\"eras\",\"eres\",\"es\",\"esa\",\"esas\",\"ese\",\"eso\",\"esos\",\"esta\",\"estaba\",\"estaban\",\"estado\",\"estados\",\"estais\",\"estamos\",\"estan\",\"estar\",\"estará\",\"estas\",\"este\",\"esto\",\"estos\",\"estoy\",\"estuvo\",\"está\",\"están\",\"ex\",\"excepto\",\"existe\",\"existen\",\"explicó\",\"expresó\",\"f\",\"fin\",\"final\",\"fue\",\"fuera\",\"fueron\",\"fui\",\"fuimos\",\"g\",\"general\",\"gran\",\"grandes\",\"gueno\",\"h\",\"ha\",\"haber\",\"habia\",\"habla\",\"hablan\",\"habrá\",\"había\",\"habían\",\"hace\",\"haceis\",\"hacemos\",\"hacen\",\"hacer\",\"hacerlo\",\"haces\",\"hacia\",\"haciendo\",\"hago\",\"han\",\"hasta\",\"hay\",\"haya\",\"he\",\"hecho\",\"hemos\",\"hicieron\",\"hizo\",\"horas\",\"hoy\",\"hubo\",\"i\",\"igual\",\"incluso\",\"indicó\",\"informo\",\"informó\",\"intenta\",\"intentais\",\"intentamos\",\"intentan\",\"intentar\",\"intentas\",\"intento\",\"ir\",\"j\",\"junto\",\"k\",\"l\",\"la\",\"lado\",\"largo\",\"las\",\"le\",\"lejos\",\"les\",\"llegó\",\"lleva\",\"llevar\",\"lo\",\"los\",\"luego\",\"lugar\",\"m\",\"mal\",\"manera\",\"manifestó\",\"mas\",\"mayor\",\"me\",\"mediante\",\"medio\",\"mejor\",\"mencionó\",\"menos\",\"menudo\",\"mi\",\"mia\",\"mias\",\"mientras\",\"mio\",\"mios\",\"mis\",\"misma\",\"mismas\",\"mismo\",\"mismos\",\"modo\",\"momento\",\"mucha\",\"muchas\",\"mucho\",\"muchos\",\"muy\",\"más\",\"mí\",\"mía\",\"mías\",\"mío\",\"míos\",\"n\",\"nada\",\"nadie\",\"ni\",\"ninguna\",\"ningunas\",\"ninguno\",\"ningunos\",\"ningún\",\"no\",\"nos\",\"nosotras\",\"nosotros\",\"nuestra\",\"nuestras\",\"nuestro\",\"nuestros\",\"nueva\",\"nuevas\",\"nuevo\",\"nuevos\",\"nunca\",\"o\",\"ocho\",\"os\",\"otra\",\"otras\",\"otro\",\"otros\",\"p\",\"pais\",\"para\",\"parece\",\"parte\",\"partir\",\"pasada\",\"pasado\",\"paìs\",\"peor\",\"pero\",\"pesar\",\"poca\",\"pocas\",\"poco\",\"pocos\",\"podeis\",\"podemos\",\"poder\",\"podria\",\"podriais\",\"podriamos\",\"podrian\",\"podrias\",\"podrá\",\"podrán\",\"podría\",\"podrían\",\"poner\",\"por\",\"porque\",\"posible\",\"primer\",\"primera\",\"primero\",\"primeros\",\"principalmente\",\"pronto\",\"propia\",\"propias\",\"propio\",\"propios\",\"proximo\",\"próximo\",\"próximos\",\"pudo\",\"pueda\",\"puede\",\"pueden\",\"puedo\",\"pues\",\"q\",\"qeu\",\"que\",\"quedó\",\"queremos\",\"quien\",\"quienes\",\"quiere\",\"quiza\",\"quizas\",\"quizá\",\"quizás\",\"quién\",\"quiénes\",\"qué\",\"r\",\"raras\",\"realizado\",\"realizar\",\"realizó\",\"repente\",\"respecto\",\"s\",\"sabe\",\"sabeis\",\"sabemos\",\"saben\",\"saber\",\"sabes\",\"salvo\",\"se\",\"sea\",\"sean\",\"segun\",\"segunda\",\"segundo\",\"según\",\"seis\",\"ser\",\"sera\",\"será\",\"serán\",\"sería\",\"señaló\",\"si\",\"sido\",\"siempre\",\"siendo\",\"siete\",\"sigue\",\"siguiente\",\"sin\",\"sino\",\"sobre\",\"sois\",\"sola\",\"solamente\",\"solas\",\"solo\",\"solos\",\"somos\",\"son\",\"soy\",\"soyos\",\"su\",\"supuesto\",\"sus\",\"suya\",\"suyas\",\"suyo\",\"sé\",\"sí\",\"sólo\",\"t\",\"tal\",\"tambien\",\"también\",\"tampoco\",\"tan\",\"tanto\",\"tarde\",\"te\",\"temprano\",\"tendrá\",\"tendrán\",\"teneis\",\"tenemos\",\"tener\",\"tenga\",\"tengo\",\"tenido\",\"tenía\",\"tercera\",\"ti\",\"tiempo\",\"tiene\",\"tienen\",\"toda\",\"todas\",\"todavia\",\"todavía\",\"todo\",\"todos\",\"total\",\"trabaja\",\"trabajais\",\"trabajamos\",\"trabajan\",\"trabajar\",\"trabajas\",\"trabajo\",\"tras\",\"trata\",\"través\",\"tres\",\"tu\",\"tus\",\"tuvo\",\"tuya\",\"tuyas\",\"tuyo\",\"tuyos\",\"tú\",\"u\",\"ultimo\",\"un\",\"una\",\"unas\",\"uno\",\"unos\",\"usa\",\"usais\",\"usamos\",\"usan\",\"usar\",\"usas\",\"uso\",\"usted\",\"ustedes\",\"v\",\"va\",\"vais\",\"valor\",\"vamos\",\"van\",\"varias\",\"varios\",\"vaya\",\"veces\",\"ver\",\"verdad\",\"verdadera\",\"verdadero\",\"vez\",\"vosotras\",\"vosotros\",\"voy\",\"vuestra\",\"vuestras\",\"vuestro\",\"vuestros\",\"w\",\"x\",\"y\",\"ya\",\"yo\",\"z\",\"él\",\"ésa\",\"ésas\",\"ése\",\"ésos\",\"ésta\",\"éstas\",\"éste\",\"éstos\",\"última\",\"últimas\",\"último\",\"últimos\",\"Ernst\",\"Ordnung\",\"Schluss\",\"a\",\"ab\",\"aber\",\"ach\",\"acht\",\"achte\",\"achten\",\"achter\",\"achtes\",\"ag\",\"alle\",\"allein\",\"allem\",\"allen\",\"aller\",\"allerdings\",\"alles\",\"allgemeinen\",\"als\",\"also\",\"am\",\"an\",\"andere\",\"anderen\",\"andern\",\"anders\",\"au\",\"auch\",\"auf\",\"aus\",\"ausser\",\"ausserdem\",\"außer\",\"außerdem\",\"b\",\"bald\",\"bei\",\"beide\",\"beiden\",\"beim\",\"beispiel\",\"bekannt\",\"bereits\",\"besonders\",\"besser\",\"besten\",\"bin\",\"bis\",\"bisher\",\"bist\",\"c\",\"d\",\"d.h\",\"da\",\"dabei\",\"dadurch\",\"dafür\",\"dagegen\",\"daher\",\"dahin\",\"dahinter\",\"damals\",\"damit\",\"danach\",\"daneben\",\"dank\",\"dann\",\"daran\",\"darauf\",\"daraus\",\"darf\",\"darfst\",\"darin\",\"darum\",\"darunter\",\"darüber\",\"das\",\"dasein\",\"daselbst\",\"dass\",\"dasselbe\",\"davon\",\"davor\",\"dazu\",\"dazwischen\",\"daß\",\"dein\",\"deine\",\"deinem\",\"deiner\",\"dem\",\"dementsprechend\",\"demgegenüber\",\"demgemäss\",\"demgemäß\",\"demselben\",\"demzufolge\",\"den\",\"denen\",\"denn\",\"denselben\",\"der\",\"deren\",\"derjenige\",\"derjenigen\",\"dermassen\",\"dermaßen\",\"derselbe\",\"derselben\",\"des\",\"deshalb\",\"desselben\",\"dessen\",\"deswegen\",\"dich\",\"die\",\"diejenige\",\"diejenigen\",\"dies\",\"diese\",\"dieselbe\",\"dieselben\",\"diesem\",\"diesen\",\"dieser\",\"dieses\",\"dir\",\"doch\",\"dort\",\"drei\",\"drin\",\"dritte\",\"dritten\",\"dritter\",\"drittes\",\"du\",\"durch\",\"durchaus\",\"durfte\",\"durften\",\"dürfen\",\"dürft\",\"e\",\"eben\",\"ebenso\",\"ehrlich\",\"ei\",\"ei,\",\"eigen\",\"eigene\",\"eigenen\",\"eigener\",\"eigenes\",\"ein\",\"einander\",\"eine\",\"einem\",\"einen\",\"einer\",\"eines\",\"einige\",\"einigen\",\"einiger\",\"einiges\",\"einmal\",\"eins\",\"elf\",\"en\",\"ende\",\"endlich\",\"entweder\",\"er\",\"erst\",\"erste\",\"ersten\",\"erster\",\"erstes\",\"es\",\"etwa\",\"etwas\",\"euch\",\"euer\",\"eure\",\"f\",\"folgende\",\"früher\",\"fünf\",\"fünfte\",\"fünften\",\"fünfter\",\"fünftes\",\"für\",\"g\",\"gab\",\"ganz\",\"ganze\",\"ganzen\",\"ganzer\",\"ganzes\",\"gar\",\"gedurft\",\"gegen\",\"gegenüber\",\"gehabt\",\"gehen\",\"geht\",\"gekannt\",\"gekonnt\",\"gemacht\",\"gemocht\",\"gemusst\",\"genug\",\"gerade\",\"gern\",\"gesagt\",\"geschweige\",\"gewesen\",\"gewollt\",\"geworden\",\"gibt\",\"ging\",\"gleich\",\"gott\",\"gross\",\"grosse\",\"grossen\",\"grosser\",\"grosses\",\"groß\",\"große\",\"großen\",\"großer\",\"großes\",\"gut\",\"gute\",\"guter\",\"gutes\",\"h\",\"habe\",\"haben\",\"habt\",\"hast\",\"hat\",\"hatte\",\"hatten\",\"hattest\",\"hattet\",\"heisst\",\"her\",\"heute\",\"hier\",\"hin\",\"hinter\",\"hoch\",\"hätte\",\"hätten\",\"i\",\"ich\",\"ihm\",\"ihn\",\"ihnen\",\"ihr\",\"ihre\",\"ihrem\",\"ihren\",\"ihrer\",\"ihres\",\"im\",\"immer\",\"in\",\"indem\",\"infolgedessen\",\"ins\",\"irgend\",\"ist\",\"j\",\"ja\",\"jahr\",\"jahre\",\"jahren\",\"je\",\"jede\",\"jedem\",\"jeden\",\"jeder\",\"jedermann\",\"jedermanns\",\"jedes\",\"jedoch\",\"jemand\",\"jemandem\",\"jemanden\",\"jene\",\"jenem\",\"jenen\",\"jener\",\"jenes\",\"jetzt\",\"k\",\"kam\",\"kann\",\"kannst\",\"kaum\",\"kein\",\"keine\",\"keinem\",\"keinen\",\"keiner\",\"kleine\",\"kleinen\",\"kleiner\",\"kleines\",\"kommen\",\"kommt\",\"konnte\",\"konnten\",\"kurz\",\"können\",\"könnt\",\"könnte\",\"l\",\"lang\",\"lange\",\"leicht\",\"leide\",\"lieber\",\"los\",\"m\",\"machen\",\"macht\",\"machte\",\"mag\",\"magst\",\"mahn\",\"mal\",\"man\",\"manche\",\"manchem\",\"manchen\",\"mancher\",\"manches\",\"mann\",\"mehr\",\"mein\",\"meine\",\"meinem\",\"meinen\",\"meiner\",\"meines\",\"mensch\",\"menschen\",\"mich\",\"mir\",\"mit\",\"mittel\",\"mochte\",\"mochten\",\"morgen\",\"muss\",\"musst\",\"musste\",\"mussten\",\"muß\",\"mußt\",\"möchte\",\"mögen\",\"möglich\",\"mögt\",\"müssen\",\"müsst\",\"müßt\",\"n\",\"na\",\"nach\",\"nachdem\",\"nahm\",\"natürlich\",\"neben\",\"nein\",\"neue\",\"neuen\",\"neun\",\"neunte\",\"neunten\",\"neunter\",\"neuntes\",\"nicht\",\"nichts\",\"nie\",\"niemand\",\"niemandem\",\"niemanden\",\"noch\",\"nun\",\"nur\",\"o\",\"ob\",\"oben\",\"oder\",\"offen\",\"oft\",\"ohne\",\"p\",\"q\",\"r\",\"recht\",\"rechte\",\"rechten\",\"rechter\",\"rechtes\",\"richtig\",\"rund\",\"s\",\"sa\",\"sache\",\"sagt\",\"sagte\",\"sah\",\"satt\",\"schlecht\",\"schon\",\"sechs\",\"sechste\",\"sechsten\",\"sechster\",\"sechstes\",\"sehr\",\"sei\",\"seid\",\"seien\",\"sein\",\"seine\",\"seinem\",\"seinen\",\"seiner\",\"seines\",\"seit\",\"seitdem\",\"selbst\",\"sich\",\"sie\",\"sieben\",\"siebente\",\"siebenten\",\"siebenter\",\"siebentes\",\"sind\",\"so\",\"solang\",\"solche\",\"solchem\",\"solchen\",\"solcher\",\"solches\",\"soll\",\"sollen\",\"sollst\",\"sollt\",\"sollte\",\"sollten\",\"sondern\",\"sonst\",\"soweit\",\"sowie\",\"später\",\"startseite\",\"statt\",\"steht\",\"suche\",\"t\",\"tag\",\"tage\",\"tagen\",\"tat\",\"teil\",\"tel\",\"tritt\",\"trotzdem\",\"tun\",\"u\",\"uhr\",\"um\",\"und\",\"und?\",\"uns\",\"unser\",\"unsere\",\"unserer\",\"unter\",\"v\",\"vergangenen\",\"viel\",\"viele\",\"vielem\",\"vielen\",\"vielleicht\",\"vier\",\"vierte\",\"vierten\",\"vierter\",\"viertes\",\"vom\",\"von\",\"vor\",\"w\",\"wahr?\",\"wann\",\"war\",\"waren\",\"wart\",\"warum\",\"was\",\"wegen\",\"weil\",\"weit\",\"weiter\",\"weitere\",\"weiteren\",\"weiteres\",\"welche\",\"welchem\",\"welchen\",\"welcher\",\"welches\",\"wem\",\"wen\",\"wenig\",\"wenige\",\"weniger\",\"weniges\",\"wenigstens\",\"wenn\",\"wer\",\"werde\",\"werden\",\"werdet\",\"weshalb\",\"wessen\",\"wie\",\"wieder\",\"wieso\",\"will\",\"willst\",\"wir\",\"wird\",\"wirklich\",\"wirst\",\"wissen\",\"wo\",\"wohl\",\"wollen\",\"wollt\",\"wollte\",\"wollten\",\"worden\",\"wurde\",\"wurden\",\"während\",\"währenddem\",\"währenddessen\",\"wäre\",\"würde\",\"würden\",\"x\",\"y\",\"z\",\"z.b\",\"zehn\",\"zehnte\",\"zehnten\",\"zehnter\",\"zehntes\",\"zeit\",\"zu\",\"zuerst\",\"zugleich\",\"zum\",\"zunächst\",\"zur\",\"zurück\",\"zusammen\",\"zwanzig\",\"zwar\",\"zwei\",\"zweite\",\"zweiten\",\"zweiter\",\"zweites\",\"zwischen\",\"zwölf\",\"über\",\"überhaupt\",\"übrigens\"])\n",
    "for each in df1_a:\n",
    "    a=str(each)\n",
    "    stop.update(word_tokenize(a.lower()))\n",
    "for er in df1_b:\n",
    "    b=str(er)\n",
    "    stop.update(word_tokenize(b.lower()))\n",
    "for we in df1_c:\n",
    "    c=str(we)\n",
    "    stop.update(word_tokenize(c.lower()))\n",
    "print(\"Stopwords are updated\")\n",
    "ai=[]\n",
    "bi=[]\n",
    "ci=[]\n",
    "di=[]\n",
    "ei=[]\n",
    "ab=[]\n",
    "aa=[]\n",
    "ac=[]\n",
    "ad=[]\n",
    "ae=[]\n",
    "ba=[]\n",
    "bb=[]\n",
    "bc=[]\n",
    "bd=[]\n",
    "af=[]\n",
    "po=place\n",
    "au=author\n",
    "plm=category\n",
    "if po == tgb:\n",
    "    aa=df_a\n",
    "    ab=df_b\n",
    "    ac=df_d\n",
    "    ad=df_e\n",
    "    ae=df_f\n",
    "elif po==wsx:\n",
    "    fr=0\n",
    "    while(fr<683395):\n",
    "        if po==str(df_c[fr]):\n",
    "            aa.append(df_a[fr])\n",
    "            ab.append(df_b[fr])\n",
    "            ac.append(df_d[fr])\n",
    "            ad.append(df_e[fr])\n",
    "            ae.append(df_f[fr])\n",
    "        fr+=1    \n",
    "i=0\n",
    "while(i<683395):\n",
    "    if po == str(df_c[i]):\n",
    "        aa.append(df_a[i])\n",
    "        ab.append(df_b[i])\n",
    "        ac.append(df_d[i])\n",
    "        ad.append(df_e[i])\n",
    "        ae.append(df_f[i])\n",
    "    i+=1\n",
    "print(\"The places have been sorted\")      \n",
    "if au==yhn:\n",
    "    ba=ab\n",
    "    bb=ac\n",
    "    bc=ad\n",
    "    bd=ae\n",
    "elif au==wsx:\n",
    "    k=0\n",
    "    while(k<(len(ab)-1)):\n",
    "        if au==str(aa[k]):\n",
    "            ba.append(ab[k])\n",
    "            bb.append(ac[k])\n",
    "            bc.append(ad[k])\n",
    "            bd.append(ae[k])\n",
    "        k+=1    \n",
    "j=0    \n",
    "while(j<(len(ab)-1)):\n",
    "    if au==str(aa[j]):\n",
    "        ba.append(ab[j])\n",
    "        bb.append(ac[j])\n",
    "        bc.append(ad[j])\n",
    "        bd.append(ae[j])\n",
    "    j+=1\n",
    "print(\"The authors have been sorted\")\n",
    "if wer==ujm:\n",
    "    ai=bb\n",
    "    bi=ba\n",
    "    ci=bd\n",
    "elif wer==wsx:\n",
    "    ikm=0\n",
    "    while(ikm<(len(ba)-1)):\n",
    "        if wer==str(bc[ikm]):\n",
    "            ai.append(bb[ikm])\n",
    "            bi.append(ba[ikm])\n",
    "            ci.append(bd[ikm])\n",
    "        ikm+=1    \n",
    "k=0    \n",
    "while(k<(len(ba)-1)):\n",
    "    if wer==str(bc[k]):\n",
    "        ai.append(bb[k])\n",
    "        bi.append(ba[k])\n",
    "        ci.append(bd[k])\n",
    "    k+=1\n",
    "print(\"The Languages have been sorted\")\n",
    "\n",
    "if choice1=='singlecategory':\n",
    "    if plm== rty:\n",
    "        di=ai\n",
    "        ei=bi\n",
    "    elif plm==wsx:\n",
    "        yhn=0\n",
    "        while(yhn<(len(bi)-1)):\n",
    "            if plm==str(ci[yhn]):\n",
    "                di.append(ai[yhn])\n",
    "                ei.append(bi[yhn])\n",
    "            yhn+=1\n",
    "    klm=0\n",
    "    while(klm<(len(bi)-1)):\n",
    "        if plm==str(ci[klm]):\n",
    "            di.append(ai[klm])\n",
    "            ei.append(bi[klm])\n",
    "        klm+=1    \n",
    "if choice1=='multicategory':\n",
    "    iuy=0\n",
    "    while(iuy<(len(bi)-1)):\n",
    "        list2=tokenizer1.tokenize(str(ci[iuy]))\n",
    "        length2=len(set(list2).intersection(input_list))\n",
    "        if (length2>0):\n",
    "            di.append(ai[iuy])\n",
    "            ei.append(bi[iuy])\n",
    "        iuy+=1\n",
    "print(\"The categories have been sorted\")    \n",
    "length1=len(ei)\n",
    "q=list(set(di))\n",
    "length=len(q)\n",
    "q.sort()\n",
    "awshesh=[]\n",
    "poiuy=[]\n",
    "iot=[]\n",
    "ytre=[]\n",
    "uioplk=[]\n",
    "aqwert=[]\n",
    "iop=[]\n",
    "def corpo(li):\n",
    "    j=0\n",
    "    while(j<(length-1)):\n",
    "        k=0\n",
    "        io=[]\n",
    "        count=0\n",
    "        while(k<length1):\n",
    "            if di[k]==li[j]:\n",
    "                io.append(ei[k])\n",
    "                count+=1\n",
    "            k+=1\n",
    "        df_m=pd.DataFrame(io,columns=['tito'])\n",
    "        df_ml=df_m['tito']\n",
    "        aw=df_ml.str.cat(sep=' ')\n",
    "        list_of_words=[i.lower() for i in wordpunct_tokenize(aw) if i.lower() not in stop and i.isalpha()]\n",
    "        import nltk\n",
    "        wordfreqdist = nltk.FreqDist(list_of_words)\n",
    "        mostcommon = wordfreqdist.most_common(10)\n",
    "        pin=[]\n",
    "        plo=[]\n",
    "        for pom in mostcommon:\n",
    "            pin.append(pom[0])\n",
    "            plo.append(pom[1]) \n",
    "        pomk=[]\n",
    "        k=0\n",
    "        while(k<length1):\n",
    "            if di[k]==li[j+1]:\n",
    "                pomk.append(ei[k])\n",
    "            k+=1\n",
    "        df_oiu=pd.DataFrame(pomk,columns=['tryu'])\n",
    "        df_gt=df_oiu['tryu']\n",
    "        aw=df_gt.str.cat(sep=' ')\n",
    "        list_of_words=[i.lower() for i in wordpunct_tokenize(aw) if i.lower() not in stop and i.isalpha()]\n",
    "        import nltk\n",
    "        wordfreqdist = nltk.FreqDist(list_of_words)\n",
    "        mostcommon1 = wordfreqdist.most_common(10)\n",
    "        oiuy=[]\n",
    "        oplikj=[]\n",
    "        for poli in mostcommon1:\n",
    "            oiuy.append(poli[0])\n",
    "            oplikj.append(poli[1])\n",
    "        concalist=oiuy+pin\n",
    "        z=0\n",
    "        plm=[]\n",
    "        okn=[]\n",
    "        def Intersection(lst1, lst2):\n",
    "            aqw=len(set(lst1).intersection(lst2))\n",
    "            aqwert.append(aqw)\n",
    "            return(aqw)\n",
    "            return set(lst1).intersection(lst2)\n",
    "        lst1 = pin\n",
    "        lst2 = oiuy\n",
    "        Intersection(lst1,lst2)\n",
    "        while(z<len(concalist)):\n",
    "            s=0\n",
    "            p=0\n",
    "            def coiuy(plkjh):\n",
    "                while(plkjh<len(pin)):\n",
    "                    if concalist[z]==pin[plkjh]:\n",
    "                        plm.append(plo[plkjh])\n",
    "                    elif (concalist[z]==pin[plkjh])==False:\n",
    "                        coiuy(plkjh+1)\n",
    "                    else:\n",
    "                        plm.append(0)\n",
    "                    plkjh+=1    \n",
    "                return plm\n",
    "            coiuy(s)\n",
    "            def uyt(edc):\n",
    "                while(edc<len(oiuy)):\n",
    "                    if concalist[z]==oiuy[edc]:\n",
    "                        okn.append(oplikj[edc])\n",
    "                    elif (concalist[z]==oiuy[edc])==False:\n",
    "                        uyt(edc+1)\n",
    "                    else:\n",
    "                        okn.append(0)\n",
    "                    edc+=1    \n",
    "                return okn\n",
    "            uyt(p)\n",
    "            z+=1                  \n",
    "        df_wwsdcv=pd.DataFrame(list(zip(plm,okn)),columns=['freq1','freq2'])\n",
    "        final=df_wwsdcv['freq1'].corr(df_wwsdcv['freq2'])\n",
    "        iop.append(count)\n",
    "        j+=1\n",
    "        awshesh.append(final)\n",
    "        \n",
    "corpo(q)\n",
    "##import csv\n",
    "##record_list = [ list(item) for item in list(zip(q,awshesh,aqwert,iop)) ]\n",
    "##with open((str(filename)+'.csv')) , \"a\", encoding= \"LATIN-1\") as fp:\n",
    "##    writer = csv.writer(fp)\n",
    "##    writer.writerows(record_list)\n",
    "print(\"The code has successfully ended . You may now open the desired file and see your expected results\")    \n",
    "        \n",
    "df_result=pd.DataFrame(list(zip(q,awshesh,aqwert,iop)),columns=['Year','Correlation','Intersection','Number of books'])\n",
    "df_result.to_csv(str(filename)+('.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import statistics\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "df=pd.read_csv(io.BytesIO(uploaded[\"ustcmasterfullyearschanged.csv\"]),low_memory=False,encoding='LATIN-1')\n",
    "place=input(\"Enter the name of place of which you want to check the freshness\")\n",
    "threshold= input(\"Enter the threshold value on the number of books\") \n",
    "##df=pd.read_csv(\"ustcmasterfullyearschanged.csv\",low_memory=False,encoding='LATIN-1')\n",
    "po=place\n",
    "df_a=df['author']\n",
    "df_b=df['title']\n",
    "df_c=df['place']\n",
    "df_d=df['year']\n",
    "df1=pd.read_csv(io.BytesIO(uploaded[\"stopwords.csv\"]), low_memory=False, encoding='LATIN-1')\n",
    "df1_a=df1['author']\n",
    "df1_b=df1['first_year']\n",
    "i=0\n",
    "aa=[]\n",
    "ab=[]\n",
    "ac=[]\n",
    "ad=[]\n",
    "ae=[]\n",
    "fr=[]\n",
    "fg=[]\n",
    "ah=[]\n",
    "while(i<683394):\n",
    "    if po == str(df_c[i]):\n",
    "        aa.append(df_a[i])\n",
    "        ab.append(df_b[i])\n",
    "        ac.append(df_d[i])\n",
    "    i+=1\n",
    "ad=list(set(aa))\n",
    "ae=list(set(ac))\n",
    "ae.sort()\n",
    "print(ae)\n",
    "y=0\n",
    "while(y<(len(ae)-1)):\n",
    "    print(ae[y])\n",
    "    f=0\n",
    "    er=[]\n",
    "    while(f<(len(ac)-1)):\n",
    "        if ae[y]==ac[f]:\n",
    "            er.append(aa[f])\n",
    "        f+=1    \n",
    "    ef=list(set(er))\n",
    "    opo=0\n",
    "    while(opo<(len(ef)-1)):\n",
    "        if 'nan' == str(ef[opo]):\n",
    "            del ef[opo]\n",
    "        opo+=1    \n",
    "    print(ef)\n",
    "    a=0\n",
    "    agelis=[]\n",
    "    if len(ef)>0:\n",
    "        while(a<(len(ef))):\n",
    "            b=0\n",
    "            while(b<(len(df1_b)-1)):\n",
    "                if ef[a]==df1_a[b]:\n",
    "                    age = int(ae[y])-int(df1_b[b])\n",
    "                    agelis.append(age)\n",
    "                b+=1\n",
    "            print(agelis)    \n",
    "            a+=1\n",
    "                    \n",
    "##    ty=0\n",
    "##    agelis=[]\n",
    "##    if (len(ef)>0):\n",
    "##        while(ty<(len(ef)-1)):\n",
    "##            op=0\n",
    "##            while(op<(len(df1_a)-1)):\n",
    "##                if ef[ty]==df1_a[op]:\n",
    "##                    age=(int(ae[y])-int(df1_b[op]))\n",
    "##                    print(age)\n",
    "##                op+=1 \n",
    "##                agelis.append(age)\n",
    "##            print(agelis)    \n",
    "##        mean_age=statistics.mean(agelis)\n",
    "##        fg.append(mean_age)\n",
    "##        print(mean_age)\n",
    "            \n",
    "    \n",
    "    fr.append(ae[y])\n",
    "    if (len(agelis)>0):\n",
    "        mean_age=statistics.mean(agelis)\n",
    "    else:\n",
    "        mean_age= 'Unlabelled Author'\n",
    "    fg.append(mean_age)    \n",
    "    y+=1\n",
    "    \n",
    "import openpyxl    \n",
    "df_result=pd.DataFrame(list(zip(fr,fg)),columns=['Year','Mean_age'])\n",
    "df_result.to_csv(\"MinimumYear3.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
