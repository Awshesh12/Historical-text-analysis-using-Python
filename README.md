# Historical-text-analysis-using-Python
Use of some machine learning and clustering tools for text analysis.
This document details the multiple techniques used to collect, organize and analyze data from a historical printing database.
# Introduction
A large data needs organization before it can be analyzed. During my winter internship I analyzed a historical dataset regarding books that were published in 15th-17th century. For preparing this dataset I scraped the data using HTML parsing from USTC catalogue (refer to code no.1 of Jupyter Notebook).The scaping was done in 3 major steps. The primary step was scraping of complete 6,83,404 books which only had the name of author, title of the book, place of printing and year of printing. For each book, I had the name of its author, place of printing and year of publication. Next, it was done by language wise and finally done by category wise. After, all these three types of datasets were created we finally merged them using stata.  In total there were 6,83,404 books out of which approx. for 5,40,000 books I had the name of categories to which the book belongs (example- Science and Mathematics, Music, Religious etc.) and 6,24,179 had the name of language in which the book was printed. So, for approx. 1,43,404 books we don’t have the number of categories on it.
# Use of Supervised and Unsupervised Machine Learning tools
I used supervised learning to categorize uncategorized data. For our analysis we have differentiated our data into forward looking and non-forward-looking books. We pick a training sample, a testing sample and our uncategorized data is the predicting sample. The variable test_ratio in the Jupyter Notebook decides what fraction of the categorized sample will be used as the testing sample for predicting accuracy.
As we have two types of books, so we could apply logistic regression for categorization of books into forward and non-forward-looking books. For logistic regression to properly work we need almost equal number of both categories. If we have the ratio of 98% and 2% of different categories, then even a random model will have a high predictive accuracy because it will give correct prediction for 98% of the data. Our data had only 12% of books that were forward looking, and even a random predictor would have had an accuracy of around 78%. When we used the title of a book as our input into the training sample (x variable) and applied logistical regression, we got an accuracy of around 88%, which was higher than random but not highly accurate. So, we couldn’t rely on the results and accuracy of this model. I also found out the results by applying Naïve Bayes model and Support Vector Machines(refer to code no.6 and 7 of Notebook). The accuracy of Naïve Bayes model was quite low whereas SVMs were time consuming.
Hence, we did something more basic. Consider the following titles.
Categorised – ‘Naufragio da nao n. senhora de Belem, feito na terra do Natal no cabo de Boa esperaní?a, & varios sucessos que teve o capití£o Joseph d...Lisboa, [por Lourení?o Craesbeeck?, 1636?].’
 Uncategorised -‘Naufragio da nao nosso senhora de Bethlem. Feito na terra do Natal no cabo de Boa esperaní?a successos que teve o capitaíµ Joseph de Ca...Lisboa, por Lourení?o Craesbeeck, 1636.’
One of the above titles is categorized and other one was uncategorized so from the titles we can figure out that the strings are almost similar and so we could use Sequence Matching from difflib module of python packages and with the help of this we can categorize some unlabelled books. Using this technique, I categorized around 18000 books, which could be categorized. The threshold of similarity that I accepted was 0.75. 
Additionally, if an author publishes a large (small) fraction of forward-looking books in a given year and in years behind and ahead, we can confidently predict that an uncategorized book by this author will be highly likely to be a forward-looking book. So, we can assign a probability of being forward-looking based on the past and future behavior of an author.
# Clustering of Phonetically same authors and places
Some of authors have been spelled differently in the dataset. So, we can cluster those authors by using Soundex similarity and string similarity. Soundex algorithm is a standard phonetic algorithm that identifies similar sounding strings. This method although provides for a quick agglomeration of similar sounding authors, but it may not always be accurate. 
We can see different names being given as the same soundex code.

Here is how the soundex algorithm works:

1)Consonants are encoded and vowel will not be encoded unless it is the first letter.

2)Save the first letter.

3)All the later occurring consonants are replaced by a number-

•	b,f,p,v – 1

•	c,g,j,k,q,s,x,z – 2   etc.

Similarly, other consonants are named. After passing it through the soundex algorithm we get a code for an input. Sometimes, even different names can get the same soundex code. For, overcoming this problem I also matched the closest strings of authors with each other and maximum string similarity ratio for every author clusters. This increased mine accuracy of results. After that we can easily give our desired names of author to a specific cluster.   
Places in master set also need to be matched with place names in another historical dataset 2 where places were differently named. So, after matching those cities that had the same name, I geocoded the location of the rest at online batchgeocoding website Doogle, and tried to match them to the nearest city in the dataset 2 which were identified by calculating the distance of every city in our main dataset to every city in dataset 2.
# Radicalness Measurement
After we have done some data cleaning and merging we can now analyze the data. Like, analyzing how the author’s way of writing books changed year after year, how the corpus of books printed at a specific location changed year after year and how the corpus of books printed in a specific language changed year and year, by looking on similarity of corpuses of titles(refer to code no.12 of Notebook). We can apply cosine similarity for finding similarity between vectorized corpuses of text. But, there are some limitations because unless a word is exactly similar to a different word both will be assigned as different vectors. 
The idea is to compare the most frequent 10 words(refer to code no.10 of Notebook) which appeared in a specific corpus of an author, in a specific place , in a particular language which belongs to a specific category in a year with the most frequent 10 words of written by same author , in same place , in same language and belongs to same category of next year. For this purpose, I initially created my own custom corpus of stopwords which had stopwords of different languages like Latin, German and French etc. so that those type of words don’t include in our most frequent words’ corpus. In addition to this, I also appended the names of authors, place and year in the stopwords dictionary. So, now after cleaning the text and making a corpus I used FreqDist class of nltk module of python to find out most frequent words and selecting top 10 of them for our precision. The same is repeated for the consecutive year . After, finding the topwords for both years I found the number of common words in both the sets. I plotted the graph of number of common words on y-axis and corresponding year on x-axis. Radicalness can be related as inversely proportional to the number of common words as- 

                Radicalness = (Maxima of the Graph ) – (y-coordinate or number of common words)
                
I considered maximum number of common words or maxima of this graph to as my minima of the Radicalness vs Year Graph shown below. This relation of radicalness and intersection of words depends on mine interpretation of radicalness. I am interpreting this as that Maximum no. of words of intersection indicates that way of writing books has not changed in that region at all. So, keeping it as the threshold of radicalness value I calculated other radicalness values.
Different interpretations of Radicalness’ relation with intersection of words will have different graphs.
# Conclusion
Data cleaning(Text Preprocessing) is a very important part of Data analysis otherwise it may lead to some misleading information which ultimately would be visible in our results. The cleaner the data is, Better and accurate are our results. Also, it becomes easier to analyze the dataset after this step. The functioning as described in the above flow chart should be used before the data goes for processing.
